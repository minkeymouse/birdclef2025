#### code_collector.py
#!/usr/bin/env python3
"""
collect_code.py – concatenate code files into a single text file

Walks the current directory and all subdirectories, finds files with common
code extensions, and writes their contents (with headers) into one output file.

Usage:
  python collect_code.py [--output ALL_CODE.txt] [--extensions .py,.js,...]
"""

import os
import argparse
from pathlib import Path

# Default extensions to include
DEFAULT_EXTS = [
    '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.hpp', '.sh', '.bat', '.yaml', '.yml', '.json', '.md'
]


def collect_code_files(root: Path, extensions: list[str]) -> list[Path]:
    """Recursively collect files under root matching the given extensions."""
    matches = []
    for dirpath, _, filenames in os.walk(root):
        for fname in filenames:
            if any(fname.endswith(ext) for ext in extensions):
                matches.append(Path(dirpath) / fname)
    return matches


def concatenate_files(files: list[Path], output: Path) -> None:
    """Write each file's path and contents into the output file."""
    with output.open('w', encoding='utf-8') as out_f:
        for fpath in sorted(files):
            out_f.write(f"#### {fpath}\n")
            try:
                text = fpath.read_text(encoding='utf-8')
            except Exception:
                # Binary or unreadable file
                out_f.write(f"[Could not read contents of {fpath}]\n\n")
                continue
            out_f.write(text)
            out_f.write("\n\n")
    print(f"Collected {len(files)} files into {output}")


def parse_args():
    p = argparse.ArgumentParser(description="Concatenate code files into one text file.")
    p.add_argument(
        '--output', '-o', type=Path, default=Path('ALL_CODE.txt'),
        help='Output file path'
    )
    p.add_argument(
        '--extensions', '-e', type=lambda s: s.split(','),
        default=DEFAULT_EXTS,
        help='Comma-separated list of file extensions to include'
    )
    p.add_argument(
        '--root', '-r', type=Path, default=Path('.'),
        help='Root directory to search'
    )
    return p.parse_args()


def main():
    args = parse_args()
    files = collect_code_files(args.root, args.extensions)
    concatenate_files(files, args.output)


if __name__ == '__main__':
    main()


#### config/inference.yaml
# Inference configuration for chunk-level prediction using trained models
paths:
  test_audio_dir: "/data/birdclef/test_soundscapes"  # directory of test soundscape audio files
  models_dir: "/data/birdclef/models"               # directory containing trained model files
  output_file: "/data/birdclef/submission.csv"      # file to save inference results (e.g. submission format)
  taxonomy_csv: "/data/birdclef/taxonomy.csv"       # taxonomy file to map class indices to species codes
inference:
  chunk_duration: 10.0        # duration of chunks to use for inference (seconds, should match training chunk length)
  chunk_hop: 5.0              # hop length for sliding window (seconds, should match training hop)
  batch_size: 16
  num_workers: 4
  smoothing_neighbors: 2      # smooth predictions by averaging over ±2 neighboring chunks in time
  presence_threshold: 0.5     # probability threshold for deciding species presence in a 5s interval
ensemble:
  checkpoints:
    - "efficientnet_b0_final_1.pth"
    - "efficientnet_b0_final_2.pth"
    - "efficientnet_b0_final_3.pth"
    - "regnety_008_final_1.pth"
    - "regnety_008_final_2.pth"
    - "regnety_008_final_3.pth"
  strategy: "average"         # ensemble strategy: average probabilities from all models


#### config/initial_train.yaml
# Training configuration for initial model training on golden labeled dataset
dataset:
  train_metadata: "/data/birdclef/processed/train_metadata.csv"
  include_pseudo: false        # do not include pseudo-labeled samples in this stage
  include_synthetic: false     # do not include synthetic samples in initial training
model:
  architectures:
    - name: "efficientnet_b0"
      num_models: 3            # train 3 EfficientNet-B0 models (e.g. different seeds)
      pretrained: true         # use ImageNet-pretrained weights
    - name: "regnety_008"
      num_models: 3            # train 3 RegNetY-0.8GF models
      pretrained: true         # use ImageNet-pretrained weights
training:
  epochs: 10
  batch_size: 32
  num_workers: 4
  seed: 42                     # base random seed for reproducibility
optimizer:
  type: "AdamW"
  lr: 0.002                    # initial learning rate
  weight_decay: 0.0001
loss: "BCEWithLogitsLoss"      # binary cross-entropy for multi-label (supports soft targets)
scheduler:
  type: "CosineAnnealingLR"
  T_max: 10                    # T_max set to number of epochs
  eta_min: 1e-6                # minimum learning rate for cosine schedule


#### config/process.yaml
# Configuration for data processing (audio cleaning, chunking, dedup, mel extraction)
paths:
  data_root: "/data/birdclef"               # root directory for BirdCLEF data
  train_csv: "/data/birdclef/train.csv"     # metadata of training audio recordings
  taxonomy_csv: "/data/birdclef/taxonomy.csv"  # (optional) taxonomy with all species codes
  audio_dir: "/data/birdclef/train_audio"   # directory containing raw training audio (subfolders per species)
  processed_dir: "/data/birdclef/processed" # directory to save processed chunks (mel and label files)
  train_metadata: "/data/birdclef/processed/train_metadata.csv"  # output CSV with training chunk metadata
  synthetic_dir: "/data/birdclef/synthetic"   # directory with synthetic audio files (if any, organized by species)
audio:
  sample_rate: 32000        # target sample rate for audio
  trim_top_db: 20           # dB threshold for trimming silence from ends
  min_duration: 5.0         # minimum duration (seconds) to keep a recording (shorter will be padded to this)
  min_rating: 1             # minimum rating to consider for inclusion
chunking:
  train_chunk_duration: 10.0    # chunk length for training data (seconds)
  train_chunk_hop: 5.0          # hop length for overlapping chunks (seconds)
  eval_segment_duration: 5.0    # duration of evaluation segments (seconds, for inference segmentation)
mel:
  n_fft: 1024             # FFT window size
  hop_length: 500         # hop length for STFT (in samples)
  n_mels: 128             # number of mel filter banks
  fmin: 40                # min frequency for mel
  fmax: 15000             # max frequency for mel
  power: 2.0              # power for magnitude (2.0 for power spectrogram)
  target_shape: [256, 256]  # target (height, width) of spectrogram after resizing
deduplication:
  enabled: true           # whether to remove duplicate audio files
  method: "hash"          # deduplication method (hash identical content)
selection:
  golden_rating: 5            # rating threshold for golden set (5-star recordings)
  require_single_label: true  # if true, only include recordings with no secondary species in golden set
  rare_species_threshold: 20  # total recordings below this count are considered rare
  synthetic_target_count: 10  # if real recordings < 10 for a species, synthetic augmentation is used
  pseudo_confidence_threshold: 0.5  # confidence threshold to accept pseudo-labeled chunks
labeling:
  use_soft_labels: true          # use soft label vectors for multi-label and pseudo-label data
  primary_label_weight: 0.7      # weight for primary species in multi-label recordings
  secondary_label_weight: 0.3    # total weight to distribute among all secondary species in a recording
  pseudo_label_weight: 0.5       # base weight for pseudo-labeled samples (lower than true labels)
  rare_label_weight: 1.0         # weight for rare-class samples (set to 1.0 so that golden/minority have full weight)


#### config/train.yaml
# Training configuration for full retraining with pseudo-labeled and synthetic data
dataset:
  train_metadata: "/data/birdclef/processed/train_metadata.csv"
  include_pseudo: true         # include pseudo-labeled samples in training
  include_synthetic: true      # include synthetic samples in training
model:
  architectures:
    - name: "efficientnet_b0"
      num_models: 3
      pretrained: true
      init_checkpoint: "/data/birdclef/models/efficientnet_b0_initial"  # base checkpoint name (will load each model)
    - name: "regnety_008"
      num_models: 3
      pretrained: true
      init_checkpoint: "/data/birdclef/models/regnety_008_initial"
training:
  epochs: 10
  batch_size: 32
  num_workers: 4
  seed: 42                     # seed (can use same as initial since different data)
optimizer:
  type: "AdamW"
  lr: 0.001                    # lower LR for fine-tuning on expanded data
  weight_decay: 0.0001
loss: "BCEWithLogitsLoss"
scheduler:
  type: "CosineAnnealingLR"
  T_max: 10
  eta_min: 1e-6


#### scripts/run_infer.sh


#### scripts/run_preprocess.sh


#### scripts/run_train.sh


#### src/__init__.py


#### src/automl_pipeline.py
# automl_pipeline.py
import os
import yaml
import pandas as pd
import numpy as np
import subprocess
from infer_utils import load_model, predict_chunks, smooth_predictions, ensemble_predictions
from metrics import create_pseudo_labels

if __name__ == "__main__":
    # Load configs
    with open("config/train.yaml", 'r') as f:
        train_config = yaml.safe_load(f)
    with open("config/inference.yaml", 'r') as f:
        infer_config = yaml.safe_load(f)
    data_root = os.path.dirname(train_config['dataset']['train_metadata'])
    # 1. Pre-processing: ensure processed data exists
    train_meta_path = train_config['dataset']['train_metadata']
    if not os.path.exists(train_meta_path):
        raise RuntimeError("Processed training metadata not found. Please run data preprocessing before pipeline.")
    # (Optional: call a preprocessing script here if needed, e.g., process.py to generate train_metadata.csv)
    # 2. Initial training (with no pseudo, maybe no synthetic)
    # Override config flags for initial run
    train_config['dataset']['include_pseudo'] = False
    train_config['dataset']['include_synthetic'] = False
    # Also possibly use a different learning rate or epochs for initial stage if desired
    initial_train_config_path = "config/_initial_train_tmp.yaml"
    with open(initial_train_config_path, 'w') as f:
        yaml.safe_dump(train_config, f)
    print("Starting initial training...")
    subprocess.run(["python", "train_efficientnet.py"], check=True)
    subprocess.run(["python", "train_regnety.py"], check=True)
    print("Initial training completed.")
    # After initial training, copy the best model weights as "initial" checkpoints for next loop (for fine-tuning).
    models_dir = infer_config['paths']['models_dir']
    eff_dir = os.path.join(models_dir, "efficientnet_b0")
    reg_dir = os.path.join(models_dir, "regnety_008")
    # Identify best checkpoints for each run (assuming train scripts print them or we pick last epoch of each run).
    # Here, for simplicity, assume the highest epoch files for run1, run2, run3 in each arch are best.
    initial_eff_ckpts = []
    initial_reg_ckpts = []
    for run in range(1, train_config['model']['architectures'][0]['num_models']+1):
        # EfficientNet
        ckpt_files = [f for f in os.listdir(eff_dir) if f.startswith("efficientnet_b0_run%d" % run)]
        if not ckpt_files:
            continue
        best_ckpt = sorted(ckpt_files, key=lambda x: int(x.split('_epoch')[-1].split('.pth')[0]))[-1]  # highest epoch
        src = os.path.join(eff_dir, best_ckpt)
        dst = os.path.join(eff_dir, f"efficientnet_b0_initial_{run}.pth")
        os.replace(src, dst)
        initial_eff_ckpts.append(dst)
        # RegNet
        ckpt_files = [f for f in os.listdir(reg_dir) if f.startswith("regnety_008_run%d" % run)]
        if not ckpt_files:
            continue
        best_ckpt = sorted(ckpt_files, key=lambda x: int(x.split('_epoch')[-1].split('.pth')[0]))[-1]
        src = os.path.join(reg_dir, best_ckpt)
        dst = os.path.join(reg_dir, f"regnety_008_initial_{run}.pth")
        os.replace(src, dst)
        initial_reg_ckpts.append(dst)
    # Update config to use these as init_checkpoint for fine-tuning
    for arch in train_config['model']['architectures']:
        if arch['name'].startswith("efficientnet"):
            arch['init_checkpoint'] = os.path.join(models_dir, "efficientnet_b0", "efficientnet_b0_initial")
        elif arch['name'].startswith("regnety"):
            arch['init_checkpoint'] = os.path.join(models_dir, "regnety_008", "regnety_008_initial")
    train_config['dataset']['include_pseudo'] = True
    train_config['dataset']['include_synthetic'] = True  # now include synthetic as well
    # Save updated config for retraining
    retrain_config_path = "config/_retrain_tmp.yaml"
    with open(retrain_config_path, 'w') as f:
        yaml.safe_dump(train_config, f)
    # 3. Iterative training loops with pseudo-label refinement
    max_loops = train_config.get('automl_loops', 5)
    for loop in range(1, max_loops):
        print(f"Starting training loop {loop} with pseudo-labels...")
        # Train models (fine-tune from initial weights including pseudo and synthetic)
        subprocess.run(["python", "train_efficientnet.py"], check=True)
        subprocess.run(["python", "train_regnety.py"], check=True)
        print(f"Loop {loop} training completed. Generating pseudo-labels for next loop...")
        # 4. Inference on training soundscapes to update pseudo-labels
        # Load the newly trained ensemble (all 6 models) for inference on unlabeled data
        ensemble_ckpts = infer_config['ensemble']['checkpoints']  # expecting final names (which we might not have yet)
        # If not explicitly set, we can derive from last training run outputs.
        # For simplicity, use initial checkpoint names (which now hold updated weights after fine-tuning)
        ensemble_ckpts = []
        for run in range(1, train_config['model']['architectures'][0]['num_models']+1):
            ensemble_ckpts.append(f"efficientnet_b0_initial_{run}.pth")
        for run in range(1, train_config['model']['architectures'][1]['num_models']+1):
            ensemble_ckpts.append(f"regnety_008_initial_{run}.pth")
        # Load models
        models = []
        for ckpt_name in ensemble_ckpts:
            if ckpt_name.startswith("efficientnet"):
                arch = "efficientnet_b0"
            else:
                arch = "regnety_008"
            ckpt_path = os.path.join(models_dir, arch, ckpt_name)
            model, species_map = load_model(arch, len(species_map) if species_map else len(class_map), ckpt_path, torch.device("cpu"))
            models.append(model)
        # Run inference on unlabeled soundscapes (assuming they are in data_root/train_soundscapes)
        unlabeled_dir = os.path.join(data_root, "train_soundscapes")
        if not os.path.isdir(unlabeled_dir):
            print("No unlabeled soundscapes directory found; skipping pseudo-label generation.")
            break
        pseudo_entries = []
        for fname in os.listdir(unlabeled_dir):
            if not fname.lower().endswith(('.wav', '.ogg', '.mp3')):
                continue
            file_path = os.path.join(unlabeled_dir, fname)
            y, sr = librosa.load(file_path, sr=None)
            # Obtain ensemble predictions for this file's 5s chunks
            all_preds = []
            for model in models:
                preds = predict_chunks(model, y, sr, chunk_duration=5.0)
                all_preds.append(preds)
            # Smooth and ensemble
            for m in range(len(all_preds)):
                all_preds[m] = smooth_predictions(all_preds[m], smoothing_window=5)
            combined = ensemble_predictions(all_preds, strategy='min_then_avg')
            # Generate pseudo-label dictionary for each chunk above threshold
            species_list = sorted(species_map.keys()) if species_map else sorted(class_map.keys())
            pseudo_labels = create_pseudo_labels(combined, species_list, threshold=presence_threshold)
            # Build DataFrame rows for these pseudo-labels
            file_id = os.path.splitext(fname)[0]
            for i, label_dict in enumerate(pseudo_labels):
                if not label_dict:
                    # If no species above threshold in this chunk, we can optionally include it as all-negative example
                    # but we might skip adding it to avoid overwhelming with negatives.
                    continue
                row = {
                    "filename": file_id,
                    "label_json": json.dumps(label_dict),
                    "weight": 0.5  # pseudo-label weight
                }
                pseudo_entries.append(row)
        # Save pseudo labels to CSV
        if pseudo_entries:
            pseudo_df = pd.DataFrame(pseudo_entries)
            pseudo_csv_path = os.path.join(data_root, "processed", "soundscape_metadata.csv")
            pseudo_df.to_csv(pseudo_csv_path, index=False)
            print(f"Pseudo-labels updated: {len(pseudo_entries)} chunks labeled and saved to {pseudo_csv_path}")
        else:
            print("No pseudo-labels generated in this iteration.")
    # 5. Final inference on test soundscapes
    print("Final training complete. Running inference on test set...")
    subprocess.run(["python", "inference.py"], check=True)
    print("AutoML pipeline finished. Submission file is ready.")


#### src/infer_utils.py
# infer_utils.py
import numpy as np
import torch
from torch import nn
import librosa

def load_model(arch_name: str, num_classes: int, checkpoint_path: str, device: torch.device):
    """Load a model architecture (EfficientNet or RegNetY) and weights from checkpoint."""
    # Create model architecture (with 1 input channel since we used mel as 1-channel expanded to 3 in data)
    if arch_name.startswith('efficientnet'):
        # EfficientNet-B0
        from torchvision import models
        model = models.efficientnet_b0(weights=None)  # no default weights, we'll load our own
        # Adjust input conv if needed (if model was trained with 3-channel input, we assume weights reflect that)
        # Our training duplicated mel to 3 channels, so model expects 3 channels.
        num_features = model.classifier[1].in_features  # EfficientNet-B0 classifier: [Dropout, Linear]
        model.classifier = nn.Linear(num_features, num_classes)
    elif arch_name.startswith('regnet'):
        from torchvision import models
        # RegNetY-800MF
        model = models.regnet_y_800mf(weights=None)
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, num_classes)
    else:
        raise ValueError(f"Unsupported architecture: {arch_name}")
    model = model.to(device)
    # Load weights
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    # Also return the species index mapping if needed
    species_map = checkpoint.get('species2idx', None)
    return model, species_map

def predict_chunks(model: nn.Module, audio, sr, chunk_duration=5.0):
    """Split audio into consecutive chunks of length chunk_duration (seconds), and return model predictions for each chunk."""
    model.eval()
    chunk_size = int(chunk_duration * sr)
    # Pad audio to have full chunks
    pad_len = (-len(audio)) % chunk_size
    if pad_len > 0:
        audio = np.concatenate([audio, np.zeros(pad_len, dtype=audio.dtype)])
    num_chunks = len(audio) // chunk_size
    preds = []
    with torch.no_grad():
        for i in range(num_chunks):
            start = i * chunk_size
            end = start + chunk_size
            y_chunk = audio[start:end]
            # Compute mel spectrogram for this chunk (same parameters as training)
            mel = librosa.feature.melspectrogram(y_chunk, sr=sr, n_fft=1024, hop_length=500, n_mels=128, fmin=40, fmax=15000)
            mel = librosa.power_to_db(mel, ref=np.max)
            # Resize mel to 128x256 if needed (assuming we want the same shape used in training)
            mel_resized = cv2.resize(mel, (256, 128))
            # Convert to 3-channel tensor
            mel_tensor = torch.from_numpy(mel_resized).float().unsqueeze(0)  # shape [1, 128, 256]
            mel_tensor = mel_tensor.unsqueeze(0)  # add channel dim -> [1, 1, 128, 256]
            mel_tensor = mel_tensor.repeat(1, 3, 1, 1)  # [1, 3, 128, 256]
            mel_tensor = mel_tensor.to(next(model.parameters()).device)
            # Forward pass
            logits = model(mel_tensor)
            prob = torch.sigmoid(logits).cpu().numpy()[0]  # probabilities for each class
            preds.append(prob)
    preds = np.array(preds)  # shape: (num_chunks, num_classes)
    return preds

def smooth_predictions(chunk_probs: np.ndarray, smoothing_window=5):
    """Apply temporal smoothing by averaging over a sliding window of `smoothing_window` consecutive chunks (centered)."""
    if smoothing_window < 2:
        return chunk_probs  # no smoothing
    half_win = smoothing_window // 2
    num_chunks, num_classes = chunk_probs.shape
    smoothed = np.zeros_like(chunk_probs)
    for t in range(num_chunks):
        # Determine window bounds
        start = max(0, t - half_win)
        end = min(num_chunks, t + half_win + 1)
        window_probs = chunk_probs[start:end]
        smoothed[t] = window_probs.mean(axis=0)
    return smoothed

def ensemble_predictions(preds_list: list, strategy: str = 'min_then_avg'):
    """
    Ensemble predictions from multiple models.
    `preds_list` is a list of numpy arrays (num_chunks x num_classes) from different models.
    strategy:
      - 'min_then_avg': take elementwise min across models of same architecture (if preds_list grouped by arch), then average between groups.
      - 'average': simple mean of all model predictions.
    """
    if strategy == 'average':
        # Simple average of all predictions
        combined = np.mean(np.stack(preds_list, axis=0), axis=0)
        return combined
    elif strategy == 'min_then_avg':
        # Assume preds_list contains groups of models from two architectures (e.g., first N from arch1, next M from arch2)
        # For robustness, we'll infer grouping by array shapes or lengths of list (if known how many per arch).
        # Here we'll assume first half of list is arch1, second half is arch2.
        k = len(preds_list) // 2
        arch1_preds = preds_list[:k]
        arch2_preds = preds_list[k:]
        # Elementwise min across models in each group
        arch1_min = np.minimum.reduce(arch1_preds)  # shape (num_chunks, num_classes)
        arch2_min = np.minimum.reduce(arch2_preds)
        # Now average the two architecture outputs
        combined = (arch1_min + arch2_min) / 2.0
        return combined
    else:
        raise ValueError(f"Unknown ensemble strategy: {strategy}")


#### src/inference.py
# inference.py
import yaml
import os
import numpy as np
import pandas as pd
import librosa
import cv2
import torch
from infer_utils import load_model, predict_chunks, smooth_predictions, ensemble_predictions

if __name__ == "__main__":
    # Load inference config
    with open("config/inference.yaml", 'r') as f:
        config = yaml.safe_load(f)
    audio_dir = config['paths']['test_audio_dir']
    models_dir = config['paths']['models_dir']
    output_file = config['paths']['output_file']
    taxonomy_path = config['paths'].get('taxonomy_csv', None)
    chunk_duration = config['inference']['chunk_duration']  # e.g., 10.0 (but competition evaluation is 5s, we'll handle splitting)
    chunk_hop = config['inference']['chunk_hop']           # e.g., 5.0
    smoothing_neighbors = config['inference'].get('smoothing_neighbors', 2)
    presence_threshold = config['inference'].get('presence_threshold', 0.5)
    ensemble_ckpts = config['ensemble']['checkpoints']
    ensemble_strategy = config['ensemble'].get('strategy', 'average')
    # Determine species list from taxonomy (for output columns)
    if taxonomy_path and os.path.exists(taxonomy_path):
        taxo_df = pd.read_csv(taxonomy_path)
        # assuming taxonomy_csv has a column 'primary_label' with species codes
        species_list = list(taxo_df['primary_label'])
    else:
        raise ValueError("Taxonomy file not found. Required to determine output classes.")
    num_classes = len(species_list)
    # Load all models
    models = []
    model_archs = []
    for ckpt_name in ensemble_ckpts:
        # Determine architecture name from checkpoint naming convention
        if ckpt_name.startswith("efficientnet"):
            arch = "efficientnet_b0"
        elif ckpt_name.startswith("regnety"):
            arch = "regnety_008"
        else:
            raise ValueError(f"Unknown model type in checkpoint name: {ckpt_name}")
        ckpt_path = os.path.join(models_dir, arch, ckpt_name)
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"Checkpoint file not found: {ckpt_path}")
        model, species_map = load_model(arch, num_classes, ckpt_path, torch.device("cpu"))
        models.append(model)
        model_archs.append(arch)
    # Iterate over each test audio file
    results = []  # to accumulate output rows
    for filename in sorted(os.listdir(audio_dir)):
        if not filename.lower().endswith(('.wav', '.ogg', '.mp3')):
            continue
        file_path = os.path.join(audio_dir, filename)
        print(f"Processing {filename}...")
        # Load full audio (assuming reasonably short soundscapes or streaming if very long)
        y, sr = librosa.load(file_path, sr=None)
        # We will perform inference on 5-second chunks (as evaluation is per 5s)
        # If chunk_duration is 10 (like training chunks), we still need 5s resolution output.
        # So we can slide a 10s window with hop=5s to get overlapping predictions, then smooth.
        # Alternatively, use the 5s base segmentation directly with our models.
        # We'll do overlapping predictions: 10s window every 5s, then combine.
        base_hop = int(5.0 * sr)
        base_chunk = int(5.0 * sr)
        # Pad audio to multiple of 5 seconds
        pad_len = (-len(y)) % base_chunk
        if pad_len > 0:
            y = np.concatenate([y, np.zeros(pad_len, dtype=y.dtype)])
        n_segments = len(y) // base_chunk
        # Ensemble predictions: we will compute using our loaded models
        # For each model, produce predictions for each 5s segment
        all_model_preds = []
        for model, arch in zip(models, model_archs):
            # If model was trained on 10s chunks, we might do a 10s sliding window:
            # But simpler: just predict on each 5s as independent (the model might generalize to 5s though trained on 10s).
            # Alternatively, if needed, we could feed two consecutive 5s together to model (as it expects 10s).
            # However, for ensemble, using overlapping windows and smoothing achieves similar effect.
            preds = predict_chunks(model, y, sr, chunk_duration=5.0)
            all_model_preds.append(preds)
        # all_model_preds is list of arrays of shape (n_segments, num_classes)
        # Ensure all predictions align in segment indexing
        # Smooth each model's predictions over time neighbors (if configured)
        if smoothing_neighbors > 0:
            for m in range(len(all_model_preds)):
                all_model_preds[m] = smooth_predictions(all_model_preds[m], smoothing_window=(2 * smoothing_neighbors + 1))
        # Apply ensemble strategy
        combined_preds = ensemble_predictions(all_model_preds, strategy=ensemble_strategy)
        # Apply chunk-level threshold to decide presence (if needed for output as binary)
        binary_preds = (combined_preds >= presence_threshold).astype(np.int_)
        # Prepare output for each 5s interval
        file_id = os.path.splitext(filename)[0]
        for i in range(n_segments):
            row_id = f"{file_id}_{i*5}"  # assuming each segment indexed by start time in seconds
            prob_row = combined_preds[i]
            # We will output probabilities for submission (the evaluation expects probabilities for each species)
            # If binary presence is needed instead, one could output binary_preds.
            result = {"row_id": row_id}
            for j, species in enumerate(species_list):
                result[species] = prob_row[j]
            results.append(result)
    # Save results to CSV
    output_df = pd.DataFrame(results)
    # Ensure columns order: row_id first, then species columns in alphabetical (or taxonomy) order
    cols = ["row_id"] + species_list
    output_df = output_df[cols]
    output_df.to_csv(output_file, index=False)
    print(f"Saved inference results to {output_file}")


#### src/metrics.py
# metrics.py
import numpy as np
from sklearn.metrics import roc_auc_score

def macro_auc_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Macro-averaged ROC-AUC, skipping classes with no true positives."""
    num_classes = y_true.shape[1]
    aucs = []
    for i in range(num_classes):
        if np.sum(y_true[:, i] == 1) == 0:
            continue  # skip classes with no positives in truth
        try:
            auc = roc_auc_score(y_true[:, i], y_pred[:, i])
            aucs.append(auc)
        except ValueError:
            continue
    return float(np.mean(aucs)) if aucs else 0.0

def macro_precision_score(y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 0.5) -> float:
    """Macro-average precision at given threshold (skip classes with no predictions and no truths)."""
    num_classes = y_true.shape[1]
    precisions = []
    for i in range(num_classes):
        pred_pos = y_pred[:, i] >= threshold
        true_pos = y_true[:, i] == 1
        if pred_pos.sum() == 0:
            if true_pos.sum() == 0:
                continue
            else:
                precisions.append(0.0)
        else:
            prec_i = np.sum(true_pos & pred_pos) / max(1, pred_pos.sum())
            precisions.append(prec_i)
    return float(np.mean(precisions)) if precisions else 0.0

def create_pseudo_labels(chunk_probs: np.ndarray, species_list: list, threshold: float = 0.5) -> list:
    """
    Given chunk-level probabilities (num_chunks x num_classes) and species list mapping indices to species codes,
    return a list of pseudo-label dicts for each chunk where probabilities exceed threshold.
    Each dict maps species code -> 1.0 for high-confidence presence.
    """
    num_chunks, num_classes = chunk_probs.shape
    pseudo_labels = []
    for t in range(num_chunks):
        label_dict = {}
        for j in range(num_classes):
            if chunk_probs[t, j] >= threshold:
                # Assign a soft label of 1.0 (presence) for that species in this chunk
                label_dict[species_list[j]] = 1.0
        pseudo_labels.append(label_dict)
    return pseudo_labels


#### src/process_gold.py
#!/usr/bin/env python3
"""
process_gold.py – Create golden training set from 5-star single-label recordings.

Filters the master training CSV for high-quality recordings (rating >= 5 with no secondary species),
then for each selected recording:
 - Loads audio, trims silence, deduplicates identical audio,
 - Splits into chunks of fixed duration (with overlap), padding if needed for short audio,
 - Extracts mel spectrogram for each chunk and saves it,
 - Creates one-hot label vector for the primary species,
 - Records metadata for each chunk in train_metadata.csv (weight=1, pseudo=0).

Run this script before initial training.
"""
import os, ast, hashlib, logging
from pathlib import Path
import yaml
import librosa
import numpy as np
import pandas as pd
from PIL import Image
import utils  # import shared utility functions

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("process_gold")

# Load configuration
with open("process.yaml", "r") as f:
    config = yaml.safe_load(f)
paths_cfg = config["paths"]
audio_cfg = config["audio"]
chunk_cfg = config["chunking"]
mel_cfg = config["mel"]
sel_cfg = config["selection"]
label_cfg = config["labeling"]
# Define important parameters
DATA_ROOT = Path(paths_cfg["data_root"])
AUDIO_DIR = Path(paths_cfg["audio_dir"])
PROCESSED_DIR = Path(paths_cfg["processed_dir"])
TRAIN_CSV = Path(paths_cfg["train_csv"])
METADATA_CSV = Path(paths_cfg["train_metadata"])
DEDUP_ENABLED = config["deduplication"]["enabled"]
# Create output directories
mel_dir = PROCESSED_DIR / "mels"
label_dir = PROCESSED_DIR / "labels"
mel_dir.mkdir(parents=True, exist_ok=True)
label_dir.mkdir(parents=True, exist_ok=True)

# Load taxonomy to get class index mapping
class_list, class_map = utils.load_taxonomy(paths_cfg.get("taxonomy_csv"), TRAIN_CSV)
num_classes = len(class_list)
logger.info(f"Loaded {num_classes} target classes.")

# Prepare to track duplicates
seen_hashes = set()
hash_file = PROCESSED_DIR / "audio_hashes.txt"
if hash_file.exists():
    # Load existing hashes (if any previous processing was done)
    with open(hash_file, 'r') as hf:
        for line in hf:
            seen_hashes.add(line.strip())

# Read training metadata CSV
train_df = pd.read_csv(TRAIN_CSV)
logger.info(f"Total recordings in train.csv: {len(train_df)}")
# Filter for golden criteria
golden_df = train_df[ (train_df["rating"] >= sel_cfg["golden_rating"]) ]
if sel_cfg.get("require_single_label", True):
    # Exclude any recording that has secondary labels listed
    if "secondary_labels" in golden_df.columns:
        # Some secondary_labels might be NaN or empty string if none
        golden_df = golden_df[golden_df["secondary_labels"].isna() | (golden_df["secondary_labels"] == "[]")]
logger.info(f"Golden set candidate recordings: {len(golden_df)}")

metadata_entries = []  # will collect rows for train_metadata
for _, row in golden_df.iterrows():
    file_id = row.get("filename") or row.get("file") or row.get("recording_id")
    primary_label = row["primary_label"]
    rating = row.get("rating", 5)
    # Only proceed if meets minimum rating (should already, but just in case)
    if rating < sel_cfg["golden_rating"]:
        continue
    # Build audio file path
    audio_path = AUDIO_DIR / primary_label / str(file_id)
    if not audio_path.exists():
        # Try adding extension if missing
        # Common extensions in dataset might be .ogg or .mp3
        if not str(audio_path).endswith(('.wav','.ogg','.mp3')):
            for ext in [".ogg", ".mp3", ".wav"]:
                if (AUDIO_DIR / primary_label / f"{file_id}{ext}").exists():
                    audio_path = AUDIO_DIR / primary_label / f"{file_id}{ext}"
                    break
    if not audio_path.exists():
        logger.warning(f"Audio file not found: {audio_path}")
        continue

    # Deduplication: check audio hash to skip identical files
    y, sr = librosa.load(audio_path, sr=audio_cfg["sample_rate"], mono=True)
    # Compute a hash of raw audio data (after resample) for dedup
    audio_hash = hashlib.md5(y.tobytes()).hexdigest()
    if DEDUP_ENABLED and audio_hash in seen_hashes:
        logger.info(f"Skipping duplicate audio: {file_id} ({primary_label})")
        continue
    seen_hashes.add(audio_hash)

    # Trim leading/trailing silence
    if audio_cfg["trim_top_db"] is not None:
        y, _ = librosa.effects.trim(y, top_db=audio_cfg["trim_top_db"])
    # Ensure minimum duration by padding or looping
    chunk_dur = int(chunk_cfg["train_chunk_duration"] * audio_cfg["sample_rate"])
    if len(y) == 0:
        continue  # skip if completely silent
    if len(y) < int(audio_cfg["min_duration"] * audio_cfg["sample_rate"]):
        # If shorter than minimum duration, loop it to reach that length
        repeat_count = int(np.ceil((audio_cfg["min_duration"] * audio_cfg["sample_rate"]) / len(y)))
        y = np.tile(y, repeat_count)[:int(audio_cfg["min_duration"] * audio_cfg["sample_rate"])]

    # Segment into overlapping chunks
    hop_samples = int(chunk_cfg["train_chunk_hop"] * audio_cfg["sample_rate"])
    total_samples = len(y)
    start = 0
    while start < total_samples:
        end = start + chunk_dur
        if end > total_samples:
            # If this is the last segment and shorter than chunk_dur, decide whether to include
            if start == 0:
                # If the whole recording is shorter than chunk duration, pad it cyclically
                if len(y) < chunk_dur:
                    repeats = (chunk_dur // len(y)) + 1
                    y_pad = np.tile(y, repeats)[:chunk_dur]
                else:
                    y_pad = y
                segment = y_pad
            else:
                # Break out if last segment would be incomplete (already covered in previous segment overlap)
                break
        else:
            segment = y[start:end]
        # Calculate chunk start time in seconds
        start_sec = start / audio_cfg["sample_rate"]
        # Compute mel spectrogram
        mel_spec = librosa.feature.melspectrogram(segment, sr=audio_cfg["sample_rate"],
                                                 n_fft=mel_cfg["n_fft"], hop_length=mel_cfg["hop_length"],
                                                 n_mels=mel_cfg["n_mels"], fmin=mel_cfg["fmin"], fmax=mel_cfg["fmax"], power=mel_cfg["power"])
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        # Resize mel spectrogram to target shape
        mel_height, mel_width = mel_db.shape
        target_h, target_w = mel_cfg["target_shape"]
        if (mel_height, mel_width) != (target_h, target_w):
            # Normalize mel values to [0,255] for image resizing
            mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-6)
            mel_img = (mel_norm * 255).astype(np.uint8)
            mel_img = Image.fromarray(mel_img)
            mel_img = mel_img.resize((target_w, target_h), Image.BILINEAR)
            mel_resized = np.array(mel_img).astype(np.float32) / 255.0
            # Convert back to dB scale range
            mel_resized = mel_resized * (mel_db.max() - mel_db.min() + 1e-6) + mel_db.min()
            mel_db = mel_resized
        # Save mel spectrogram array
        chunk_id = utils.hash_chunk_id(str(file_id), start_sec)
        mel_path = mel_dir / f"{chunk_id}.npy"
        label_path = label_dir / f"{chunk_id}.npy"
        np.save(mel_path, mel_db.astype(np.float32))
        # Create label vector (one-hot since no secondary labels)
        label_vec = np.zeros(num_classes, dtype=np.float32)
        label_index = class_map.get(primary_label)
        if label_index is None:
            # If species not in our class map (should not happen if taxonomy covers all)
            logger.warning(f"Species {primary_label} not in class map, skipping chunk.")
            start += hop_samples
            continue
        label_vec[label_index] = 1.0  # one-hot for primary species
        np.save(label_path, label_vec)
        # Add metadata entry
        metadata_entries.append({
            "filename": str(file_id),
            "start_sec": round(start_sec, 3),
            "mel_path": str(mel_path),
            "label_path": str(label_path),
            "weight": 1.0,            # golden samples weight=1
            "pseudo": 0               # human-labeled
        })
        start += hop_samples
    logger.info(f"Processed golden file: {file_id} ({primary_label})")

# Save metadata to CSV
md_df = pd.DataFrame(metadata_entries, columns=["filename","start_sec","mel_path","label_path","weight","pseudo"])
md_df.to_csv(METADATA_CSV, index=False)
logger.info(f"Golden set processing complete. Chunks saved: {len(md_df)}. Metadata written to {METADATA_CSV}")

# Save updated hash list for deduplication reference
if DEDUP_ENABLED:
    with open(hash_file, 'w') as hf:
        for h in seen_hashes:
            hf.write(f"{h}\n")


#### src/process_pseudo.py
#!/usr/bin/env python3
"""
process_pseudo.py – Generate pseudo-labels for remaining unlabeled recordings.

Uses the initially trained models to predict labels for all recordings not already in the training set.
For each recording not in train_metadata (i.e., not processed in golden or rare stages):
 - Splits the recording into chunks (with overlap) for inference,
 - Averages predictions from all available models (ensemble),
 - For each chunk where the top prediction confidence >= threshold, saves that chunk with the model-provided label probabilities as soft labels,
 - Marks these chunks as pseudo-labeled with a lower weight,
 - Appends the new entries to train_metadata.csv.

Run after initial model training to expand the training dataset with pseudo-labeled data.
"""
import os, hashlib, logging
from pathlib import Path
import yaml
import librosa
import numpy as np
import pandas as pd
import torch
import utils

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("process_pseudo")

# Load config
with open("process.yaml", "r") as f:
    config = yaml.safe_load(f)
paths_cfg = config["paths"]
audio_cfg = config["audio"]
chunk_cfg = config["chunking"]
mel_cfg = config["mel"]
sel_cfg = config["selection"]
label_cfg = config["labeling"]
DATA_ROOT = Path(paths_cfg["data_root"])
AUDIO_DIR = Path(paths_cfg["audio_dir"])
PROCESSED_DIR = Path(paths_cfg["processed_dir"])
TRAIN_CSV = Path(paths_cfg["train_csv"])
METADATA_CSV = Path(paths_cfg["train_metadata"])
models_dir = Path("/data/birdclef/models")  # directory where initial models are stored (from initial_train stage)

# Load class map and list
class_list, class_map = utils.load_taxonomy(paths_cfg.get("taxonomy_csv"), TRAIN_CSV)
num_classes = len(class_list)

# Load train_metadata to find used files
if METADATA_CSV.exists():
    used_files = set(pd.read_csv(METADATA_CSV)["filename"].astype(str).unique())
else:
    used_files = set()

# Prepare models for inference (load all initial trained model checkpoints)
# We assume initial models are saved with a known naming scheme (as per initial_train.yaml config)
ensemble_models = []
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Model architecture definitions (match those in initial_train.yaml)
initial_archs = [("efficientnet_b0", 3), ("regnety_008", 3)]
for arch_name, count in initial_archs:
    for i in range(1, count+1):
        ckpt_path = models_dir / f"{arch_name}_initial_{i}.pth"
        if not ckpt_path.exists():
            logger.warning(f"Model checkpoint not found: {ckpt_path}")
            continue
        # Load model (assuming torchscript or state dict saved)
        try:
            model = torch.jit.load(str(ckpt_path)) if ckpt_path.suffix == ".pt" else None
        except Exception:
            model = None
        if model is None:
            # If saved as state dict, you'd need model class definitions to load. 
            # Here we assume model was saved as torchscript or entire model.
            logger.warning(f"Unable to load model (requires definition): {ckpt_path.name}")
            continue
        model.eval().to(device)
        ensemble_models.append(model)
logger.info(f"Loaded {len(ensemble_models)} models for pseudo-label inference.")

if not ensemble_models:
    logger.error("No models loaded for inference. Aborting pseudo-labeling.")
    exit(1)

# Deduplication: prepare seen hashes set including all already used files
seen_hashes = set()
hash_file = PROCESSED_DIR / "audio_hashes.txt"
if hash_file.exists():
    with open(hash_file, 'r') as hf:
        for line in hf:
            seen_hashes.add(line.strip())

new_entries = []
# Iterate over all recordings in train.csv that were not used in training
train_df = pd.read_csv(TRAIN_CSV)
for _, row in train_df.iterrows():
    file_id = str(row.get("filename") or row.get("file") or row.get("recording_id"))
    primary_label = row["primary_label"]
    if file_id in used_files:
        continue
    audio_path = AUDIO_DIR / primary_label / file_id
    if not audio_path.exists():
        # try with extension
        found = False
        for ext in [".ogg", ".mp3", ".wav"]:
            if (AUDIO_DIR / primary_label / f"{file_id}{ext}").exists():
                audio_path = AUDIO_DIR / primary_label / f"{file_id}{ext}"
                found = True
                break
        if not found:
            logger.warning(f"File not found for pseudo-labeling: {file_id}")
            continue

    # Dedup check
    y, sr = librosa.load(audio_path, sr=audio_cfg["sample_rate"], mono=True)
    audio_hash = hashlib.md5(y.tobytes()).hexdigest()
    if config["deduplication"]["enabled"] and audio_hash in seen_hashes:
        logger.info(f"Skipping duplicate (unlabeled) audio: {file_id}")
        continue
    seen_hashes.add(audio_hash)

    # Trim ends silence
    if audio_cfg["trim_top_db"] is not None:
        y, _ = librosa.effects.trim(y, top_db=audio_cfg["trim_top_db"])
    if len(y) == 0:
        continue

    # Chunk for inference (use same chunk length and hop as training)
    chunk_samples = int(chunk_cfg["train_chunk_duration"] * audio_cfg["sample_rate"])
    hop_samples = int(chunk_cfg["train_chunk_hop"] * audio_cfg["sample_rate"])
    total_samples = len(y)
    # Pad the audio at end to cover last segment fully
    if total_samples < chunk_samples:
        # pad with silence to one chunk length
        pad = chunk_samples - total_samples
        y = np.concatenate([y, np.zeros(pad, dtype=y.dtype)])
        total_samples = len(y)
    elif total_samples % hop_samples != 0:
        # pad so that the last start index aligns to cover end
        pad = hop_samples - (total_samples % hop_samples)
        y = np.concatenate([y, np.zeros(pad, dtype=y.dtype)])
        total_samples = len(y)

    # Slide through audio
    start = 0
    while start < total_samples:
        end = start + chunk_samples
        if end > total_samples:
            segment = y[start:total_samples]
            # pad this last segment to full length
            segment = np.pad(segment, (0, end - total_samples), mode='constant')
        else:
            segment = y[start:end]
        start_sec = start / audio_cfg["sample_rate"]
        # Compute mel spectrogram for the segment
        mel_spec = librosa.feature.melspectrogram(segment, sr=audio_cfg["sample_rate"],
                                                 n_fft=mel_cfg["n_fft"], hop_length=mel_cfg["hop_length"],
                                                 n_mels=mel_cfg["n_mels"], fmin=mel_cfg["fmin"], fmax=mel_cfg["fmax"], power=mel_cfg["power"])
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        # Resize mel to target shape
        h, w = mel_db.shape
        target_h, target_w = mel_cfg["target_shape"]
        if (h, w) != (target_h, target_w):
            mel_db = utils.resize_mel(mel_db, target_h, target_w)
        # Convert mel to torch tensor and predict with ensemble
        mel_tensor = torch.from_numpy(mel_db).unsqueeze(0).unsqueeze(0).to(device)  # shape (1,1,H,W)
        # If model expects 3-channel input, we would stack mel 3 times here. Assume our models were adapted to 1-channel.
        avg_pred = np.zeros(num_classes, dtype=np.float32)
        with torch.no_grad():
            for model in ensemble_models:
                # Each model should output a tensor of shape (1, num_classes) logits
                logits = model(mel_tensor)
                probs = torch.sigmoid(logits).cpu().numpy().flatten()  # convert to probabilities
                avg_pred += probs.astype(np.float32)
        avg_pred /= len(ensemble_models)
        # Determine if this chunk has a confident prediction
        top_prob = float(np.max(avg_pred))
        if top_prob >= sel_cfg["pseudo_confidence_threshold"]:
            # Save chunk and pseudo label
            chunk_id = utils.hash_chunk_id(file_id, start_sec)
            mel_path = PROCESSED_DIR / "mels" / f"{chunk_id}.npy"
            label_path = PROCESSED_DIR / "labels" / f"{chunk_id}.npy"
            np.save(mel_path, mel_db.astype(np.float32))
            # Save soft label vector (predicted probabilities)
            label_vec = avg_pred  # already numpy array of float32
            np.save(label_path, label_vec.astype(np.float32))
            # Assign weight (pseudo labels have lower weight)
            w = label_cfg["pseudo_label_weight"]
            # Optionally scale weight by confidence (not explicitly configured, so skipping)
            new_entries.append({
                "filename": file_id,
                "start_sec": round(start_sec, 3),
                "mel_path": str(mel_path),
                "label_path": str(label_path),
                "weight": float(w),
                "pseudo": 1
            })
        start += hop_samples
    logger.info(f"Inferred pseudo-labels for file: {file_id}")

# Append pseudo-labeled entries to metadata
if new_entries:
    md_df = pd.DataFrame(new_entries, columns=["filename","start_sec","mel_path","label_path","weight","pseudo"])
    md_df.to_csv(METADATA_CSV, mode='a', index=False, header=False)
    logger.info(f"Appended {len(md_df)} pseudo-labeled chunks to train_metadata.csv.")
else:
    logger.info("No pseudo-labeled chunks added (no confident predictions).")

# Update dedup hash record
if config["deduplication"]["enabled"]:
    with open(PROCESSED_DIR / "audio_hashes.txt", 'w') as hf:
        for h in seen_hashes:
            hf.write(f"{h}\n")


#### src/process_rare.py
#!/usr/bin/env python3
"""
process_rare.py – Add rare classes to training set by including lower-rated recordings.

Identifies bird species with few available recordings (based on train.csv and a threshold).
For each rare species, includes all recordings of that species (down to min_rating) that were not in the golden set.
Processes each new recording similar to process_gold (trimming, chunking, mel extraction).
Also incorporates synthetic audio for those species if available (from synthetic_dir).
Appends the new chunks to the existing train_metadata.csv with appropriate labels and weights.
Run this after process_gold (and initial training, if desired) to enrich the training data with minority classes.
"""
import os, ast, hashlib, logging
from pathlib import Path
import yaml
import librosa
import numpy as np
import pandas as pd
from PIL import Image
import utils

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("process_rare")

# Load configuration
with open("process.yaml", "r") as f:
    config = yaml.safe_load(f)
paths_cfg = config["paths"]
audio_cfg = config["audio"]
chunk_cfg = config["chunking"]
mel_cfg = config["mel"]
sel_cfg = config["selection"]
label_cfg = config["labeling"]
DATA_ROOT = Path(paths_cfg["data_root"])
AUDIO_DIR = Path(paths_cfg["audio_dir"])
SYN_DIR = Path(paths_cfg.get("synthetic_dir", ""))
PROCESSED_DIR = Path(paths_cfg["processed_dir"])
TRAIN_CSV = Path(paths_cfg["train_csv"])
METADATA_CSV = Path(paths_cfg["train_metadata"])
DEDUP_ENABLED = config["deduplication"]["enabled"]

mel_dir = PROCESSED_DIR / "mels"
label_dir = PROCESSED_DIR / "labels"
mel_dir.mkdir(parents=True, exist_ok=True)
label_dir.mkdir(parents=True, exist_ok=True)

# Load class mapping
class_list, class_map = utils.load_taxonomy(paths_cfg.get("taxonomy_csv"), TRAIN_CSV)
num_classes = len(class_list)

# Prepare deduplication set
seen_hashes = set()
hash_file = PROCESSED_DIR / "audio_hashes.txt"
if hash_file.exists():
    with open(hash_file, 'r') as hf:
        for line in hf:
            seen_hashes.add(line.strip())

# Determine rare species based on occurrence counts
train_df = pd.read_csv(TRAIN_CSV)
# Count total recordings per species (all ratings)
species_counts = train_df.groupby("primary_label")["filename"].count().to_dict()
rare_species = [sp for sp, count in species_counts.items() if count < sel_cfg["rare_species_threshold"]]
logger.info(f"Identified {len(rare_species)} rare species with < {sel_cfg['rare_species_threshold']} recordings.")

# Also ensure we only add species that were not fully covered in golden
# Actually, if a rare species had some golden entries, we may still add more of its recordings
# So no need to exclude those, we will skip duplicates by file name or hash anyway.
# Load existing train_metadata to skip files already processed
existing_md = pd.read_csv(METADATA_CSV)
used_files = set(existing_md["filename"].astype(str).unique())

new_entries = []
# Process real recordings for rare species
for species in rare_species:
    # Find all recordings of this species in train_df
    species_df = train_df[train_df["primary_label"] == species]
    if species_df.empty:
        continue
    for _, row in species_df.iterrows():
        file_id = row.get("filename") or row.get("file") or row.get("recording_id")
        if str(file_id) in used_files:
            # Already included in golden set
            continue
        rating = row.get("rating", 0)
        if rating < audio_cfg["min_rating"]:
            continue
        secondary_labels = []
        if "secondary_labels" in row and isinstance(row["secondary_labels"], str) and row["secondary_labels"].strip():
            # Parse secondary labels list from string
            try:
                secondary_labels = ast.literal_eval(row["secondary_labels"])
            except Exception:
                secondary_labels = []
        audio_path = AUDIO_DIR / species / str(file_id)
        if not audio_path.exists():
            if not str(audio_path).endswith(('.wav','.ogg','.mp3')):
                for ext in [".ogg", ".mp3", ".wav"]:
                    if (AUDIO_DIR / species / f"{file_id}{ext}").exists():
                        audio_path = AUDIO_DIR / species / f"{file_id}{ext}"
                        break
        if not audio_path.exists():
            logger.warning(f"Audio file not found: {audio_path}")
            continue

        # Deduplication check
        y, sr = librosa.load(audio_path, sr=audio_cfg["sample_rate"], mono=True)
        audio_hash = hashlib.md5(y.tobytes()).hexdigest()
        if DEDUP_ENABLED and audio_hash in seen_hashes:
            logger.info(f"Skipping duplicate audio: {file_id} ({species})")
            continue
        seen_hashes.add(audio_hash)

        # Trim silence from ends
        if audio_cfg["trim_top_db"] is not None:
            y, _ = librosa.effects.trim(y, top_db=audio_cfg["trim_top_db"])
        if len(y) == 0:
            continue
        # Pad short recordings to min_duration if needed
        if len(y) < int(audio_cfg["min_duration"] * audio_cfg["sample_rate"]):
            repeats = int(np.ceil((audio_cfg["min_duration"] * audio_cfg["sample_rate"]) / len(y)))
            y = np.tile(y, repeats)[:int(audio_cfg["min_duration"] * audio_cfg["sample_rate"])]

        # Chunk into segments
        chunk_samples = int(chunk_cfg["train_chunk_duration"] * audio_cfg["sample_rate"])
        hop_samples = int(chunk_cfg["train_chunk_hop"] * audio_cfg["sample_rate"])
        total_samples = len(y)
        start = 0
        while start < total_samples:
            end = start + chunk_samples
            if end > total_samples:
                if start == 0:
                    # file shorter than one chunk
                    if len(y) < chunk_samples:
                        repeats = (chunk_samples // len(y)) + 1
                        seg = np.tile(y, repeats)[:chunk_samples]
                    else:
                        seg = y
                else:
                    break
            else:
                seg = y[start:end]
            start_sec = start / audio_cfg["sample_rate"]
            # Compute mel spectrogram
            mel_spec = librosa.feature.melspectrogram(seg, sr=audio_cfg["sample_rate"],
                                                     n_fft=mel_cfg["n_fft"], hop_length=mel_cfg["hop_length"],
                                                     n_mels=mel_cfg["n_mels"], fmin=mel_cfg["fmin"], fmax=mel_cfg["fmax"], power=mel_cfg["power"])
            mel_db = librosa.power_to_db(mel_spec, ref=np.max)
            # Resize mel to target shape
            mel_height, mel_width = mel_db.shape
            target_h, target_w = mel_cfg["target_shape"]
            if (mel_height, mel_width) != (target_h, target_w):
                mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-6)
                mel_img = Image.fromarray((mel_norm*255).astype(np.uint8))
                mel_img = mel_img.resize((target_w, target_h), Image.BILINEAR)
                mel_resized = np.array(mel_img).astype(np.float32) / 255.0
                mel_resized = mel_resized * (mel_db.max() - mel_db.min() + 1e-6) + mel_db.min()
                mel_db = mel_resized
            # Save mel
            chunk_id = utils.hash_chunk_id(str(file_id), start_sec)
            mel_path = mel_dir / f"{chunk_id}.npy"
            label_path = label_dir / f"{chunk_id}.npy"
            np.save(mel_path, mel_db.astype(np.float32))
            # Create label vector (may be multi-label if secondary species present)
            prim_label = species
            sec_labels = secondary_labels if label_cfg["use_soft_labels"] and secondary_labels else []
            label_vec = utils.create_label_vector(prim_label, sec_labels, class_map,
                                                  primary_weight=label_cfg["primary_label_weight"],
                                                  secondary_weight=label_cfg["secondary_label_weight"],
                                                  use_soft=label_cfg["use_soft_labels"])
            np.save(label_path, label_vec.astype(np.float32))
            # weight for rare class sample
            w = label_cfg.get("rare_label_weight", 1.0)
            new_entries.append({
                "filename": str(file_id),
                "start_sec": round(start_sec, 3),
                "mel_path": str(mel_path),
                "label_path": str(label_path),
                "weight": float(w),
                "pseudo": 0
            })
            start += hop_samples
        logger.info(f"Processed rare file: {file_id} (species {species})")

# Process synthetic audio for rare species (if any synthetic files exist)
if SYN_DIR and Path(SYN_DIR).exists():
    syn_files = list(Path(SYN_DIR).rglob("*.*"))
    logger.info(f"Found {len(syn_files)} synthetic audio files.")
    for syn_path in syn_files:
        species = syn_path.parent.name  # assuming synthetic files are in subdirectories named by species
        if species not in rare_species:
            # Only include synthetic for species truly rare or needed
            # (Optionally could include synthetic for others if provided, but skip if not rare)
            continue
        file_id = syn_path.stem
        # Dedup check: synthetic likely unique, but check to avoid duplicates among synthetic
        y, sr = librosa.load(syn_path, sr=audio_cfg["sample_rate"], mono=True)
        audio_hash = hashlib.md5(y.tobytes()).hexdigest()
        if DEDUP_ENABLED and audio_hash in seen_hashes:
            logger.info(f"Skipping duplicate synthetic audio: {syn_path.name}")
            continue
        seen_hashes.add(audio_hash)
        # Trim silence (if any) and pad if needed
        if audio_cfg["trim_top_db"] is not None:
            y, _ = librosa.effects.trim(y, top_db=audio_cfg["trim_top_db"])
        if len(y) == 0:
            continue
        if len(y) < int(audio_cfg["min_duration"] * audio_cfg["sample_rate"]):
            repeats = int(np.ceil((audio_cfg["min_duration"] * audio_cfg["sample_rate"]) / len(y)))
            y = np.tile(y, repeats)[:int(audio_cfg["min_duration"] * audio_cfg["sample_rate"])]

        # For synthetic, many are short single calls; we'll create at least one chunk (pad to full chunk_dur)
        chunk_samples = int(chunk_cfg["train_chunk_duration"] * audio_cfg["sample_rate"])
        if len(y) < chunk_samples:
            repeats = (chunk_samples // len(y)) + 1
            y = np.tile(y, repeats)[:chunk_samples]
        total_samples = len(y)
        start = 0
        while start < total_samples:
            end = start + chunk_samples
            if end > total_samples:
                segment = y[start:]  # last segment (should not happen if we padded to chunk length)
            else:
                segment = y[start:end]
            start_sec = start / audio_cfg["sample_rate"]
            mel_spec = librosa.feature.melspectrogram(segment, sr=audio_cfg["sample_rate"],
                                                     n_fft=mel_cfg["n_fft"], hop_length=mel_cfg["hop_length"],
                                                     n_mels=mel_cfg["n_mels"], fmin=mel_cfg["fmin"], fmax=mel_cfg["fmax"], power=mel_cfg["power"])
            mel_db = librosa.power_to_db(mel_spec, ref=np.max)
            mel_height, mel_width = mel_db.shape
            target_h, target_w = mel_cfg["target_shape"]
            if (mel_height, mel_width) != (target_h, target_w):
                mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-6)
                mel_img = Image.fromarray((mel_norm*255).astype(np.uint8))
                mel_img = mel_img.resize((target_w, target_h), Image.BILINEAR)
                mel_resized = np.array(mel_img).astype(np.float32) / 255.0
                mel_resized = mel_resized * (mel_db.max() - mel_db.min() + 1e-6) + mel_db.min()
                mel_db = mel_resized
            chunk_id = utils.hash_chunk_id(syn_path.stem, start_sec)
            mel_path = mel_dir / f"{chunk_id}.npy"
            label_path = label_dir / f"{chunk_id}.npy"
            np.save(mel_path, mel_db.astype(np.float32))
            # Label: one-hot for synthetic (single target species)
            label_vec = np.zeros(num_classes, dtype=np.float32)
            if species in class_map:
                label_vec[class_map[species]] = 1.0
            else:
                logger.warning(f"Synthetic species {species} not in class map, skipping.")
                start += chunk_samples
                continue
            np.save(label_path, label_vec)
            w = label_cfg.get("rare_label_weight", 1.0)  # treat synthetic similar to rare real
            new_entries.append({
                "filename": syn_path.name,
                "start_sec": round(start_sec, 3),
                "mel_path": str(mel_path),
                "label_path": str(label_path),
                "weight": float(w),
                "pseudo": 0
            })
            start += chunk_samples
        logger.info(f"Processed synthetic file: {syn_path.name} (species {species})")

# Append new entries to train_metadata.csv
if new_entries:
    md_df = pd.DataFrame(new_entries, columns=["filename","start_sec","mel_path","label_path","weight","pseudo"])
    md_df.to_csv(METADATA_CSV, mode='a', index=False, header=False)
    logger.info(f"Added {len(md_df)} new entries to train_metadata.csv for rare species.")
else:
    logger.info("No new entries added for rare species.")
# Update hash file
if DEDUP_ENABLED:
    with open(hash_file, 'w') as hf:
        for h in seen_hashes:
            hf.write(f"{h}\n")


#### src/train_efficientnet.py
# train_efficientnet.py
import yaml
import json
import torch
from train_utils import BirdClefDataset, create_dataloader, train_model

if __name__ == "__main__":
    # Load training config
    with open("config/train.yaml", 'r') as f:
        config = yaml.safe_load(f)
    # Identify EfficientNet config
    arch_config = None
    for arch in config['model']['architectures']:
        if arch['name'].startswith("efficientnet"):
            arch_config = arch
            break
    if arch_config is None:
        raise ValueError("EfficientNet configuration not found in train.yaml")
    model_name = arch_config['name']
    num_models = arch_config.get('num_models', 1)
    pretrained = arch_config.get('pretrained', True)
    # Load training metadata
    train_meta_path = config['dataset']['train_metadata']
    df = pd.read_csv(train_meta_path)
    # If pseudo-labeled samples should be included and exist, append them
    if config['dataset'].get('include_pseudo', False):
        pseudo_meta_path = os.path.join(os.path.dirname(train_meta_path), "soundscape_metadata.csv")
        if os.path.exists(pseudo_meta_path):
            df_pseudo = pd.read_csv(pseudo_meta_path)
            # We assume pseudo metadata already in same format (including label_json and weight)
            df = pd.concat([df, df_pseudo], ignore_index=True)
    # If synthetic samples should be included, they are assumed to be already in train_metadata or processed.
    # (If synthetic are separate, one could similarly read and append them here.)
    # Create class map (species to index)
    if 'label_json' in df.columns:
        # Build class set from all labels present
        species_set = {sp for js in df['label_json'] for sp in json.loads(js).keys()}
    else:
        # If no label_json, assume a 'primary_label' column exists
        species_set = set(df['primary_label'].unique())
        # Also include any secondary labels if provided
        if 'secondary_labels' in df.columns:
            for sec_list in df['secondary_labels']:
                if isinstance(sec_list, str):
                    for sp in sec_list.split():
                        species_set.add(sp)
    species_list = sorted(species_set)
    class_map = {sp: idx for idx, sp in enumerate(species_list)}
    num_classes = len(class_map)
    # Split train/val (for simplicity, use a small percentage as validation)
    val_frac = config['training'].get('val_fraction', 0.1)
    df_val = df.sample(frac=val_frac, random_state=42)
    df_train = df.drop(df_val.index).reset_index(drop=True)
    df_val = df_val.reset_index(drop=True)
    # Create datasets
    train_dataset = BirdClefDataset(df_train, class_map, mel_shape=(128, 256), augment=True)
    val_dataset = BirdClefDataset(df_val, class_map, mel_shape=(128, 256), augment=False)
    train_loader = create_dataloader(train_dataset, batch_size=config['training']['batch_size'], shuffle=False)
    val_loader = DataLoader(val_dataset, batch_size=config['training']['batch_size'], shuffle=False, num_workers=4)
    # Set device (use GPU if available)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Train specified number of models (e.g., 3 seeds for ensemble)
    saved_checkpoints = []
    base_seed = config['training'].get('seed', 42)
    for run in range(1, num_models+1):
        # Set reproducibility seed
        torch.manual_seed(base_seed + run)
        np.random.seed(base_seed + run)
        # Initialize model
        from torchvision import models
        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT if pretrained else None)
        # Adjust final layer for num_classes
        num_feats = model.classifier[1].in_features
        model.classifier = torch.nn.Linear(num_feats, num_classes)
        model = model.to(device)
        # If fine-tuning from a provided initial checkpoint (for pseudo label iterations)
        init_ckpt_base = arch_config.get('init_checkpoint', None)
        if init_ckpt_base:
            ckpt_path = f"{init_ckpt_base}_{run}.pth"
            if os.path.exists(ckpt_path):
                ckpt = torch.load(ckpt_path, map_location=device)
                model.load_state_dict(ckpt['model_state_dict'])
                print(f"Loaded initial checkpoint for {model_name} run{run} from {ckpt_path}")
        # Prepare config for train_model
        run_config = config.copy()
        run_config['current_arch'] = model_name
        run_config['current_run'] = run
        run_config['class_map'] = class_map
        run_config['paths'] = {
            'models_dir': os.path.join("models", model_name)  # save in models/<architecture> folder
        }
        os.makedirs(run_config['paths']['models_dir'], exist_ok=True)
        # Train model
        best_ckpts = train_model(model, train_loader, val_loader, run_config, device)
        saved_checkpoints.extend(best_ckpts)
    # After training, output the list of saved checkpoint paths
    print("Training complete. Saved model checkpoints:")
    for ckpt in saved_checkpoints:
        print(" -", ckpt)


#### src/train_regnety.py
# train_regnety.py
import yaml
import json
import torch
import numpy as np
import pandas as pd
import os
from train_utils import BirdClefDataset, create_dataloader, train_model

if __name__ == "__main__":
    # Load training config
    with open("config/train.yaml", 'r') as f:
        config = yaml.safe_load(f)
    # Identify RegNet config
    arch_config = None
    for arch in config['model']['architectures']:
        if arch['name'].startswith("regnet"):
            arch_config = arch
            break
    if arch_config is None:
        raise ValueError("RegNetY configuration not found in train.yaml")
    model_name = arch_config['name']
    num_models = arch_config.get('num_models', 1)
    pretrained = arch_config.get('pretrained', True)
    # Load training metadata (same process as in train_efficientnet.py)
    train_meta_path = config['dataset']['train_metadata']
    df = pd.read_csv(train_meta_path)
    if config['dataset'].get('include_pseudo', False):
        pseudo_meta_path = os.path.join(os.path.dirname(train_meta_path), "soundscape_metadata.csv")
        if os.path.exists(pseudo_meta_path):
            df_pseudo = pd.read_csv(pseudo_meta_path)
            df = pd.concat([df, df_pseudo], ignore_index=True)
    # (Assume synthetic already in df if include_synthetic true)
    # Create class map
    if 'label_json' in df.columns:
        species_set = {sp for js in df['label_json'] for sp in json.loads(js).keys()}
    else:
        species_set = set(df['primary_label'].unique())
        if 'secondary_labels' in df.columns:
            for sec_list in df['secondary_labels']:
                if isinstance(sec_list, str):
                    for sp in sec_list.split():
                        species_set.add(sp)
    species_list = sorted(species_set)
    class_map = {sp: idx for idx, sp in enumerate(species_list)}
    num_classes = len(class_map)
    # Train/val split
    val_frac = config['training'].get('val_fraction', 0.1)
    df_val = df.sample(frac=val_frac, random_state=42)
    df_train = df.drop(df_val.index).reset_index(drop=True)
    df_val = df_val.reset_index(drop=True)
    # Datasets and loaders
    train_dataset = BirdClefDataset(df_train, class_map, mel_shape=(128, 256), augment=True)
    val_dataset = BirdClefDataset(df_val, class_map, mel_shape=(128, 256), augment=False)
    train_loader = create_dataloader(train_dataset, batch_size=config['training']['batch_size'], shuffle=False)
    val_loader = DataLoader(val_dataset, batch_size=config['training']['batch_size'], shuffle=False, num_workers=4)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    saved_checkpoints = []
    base_seed = config['training'].get('seed', 42)
    for run in range(1, num_models+1):
        torch.manual_seed(base_seed + run)
        np.random.seed(base_seed + run)
        from torchvision import models
        model = models.regnet_y_800mf(weights=models.RegNet_Y_800MF_Weights.DEFAULT if pretrained else None)
        num_feats = model.fc.in_features
        model.fc = torch.nn.Linear(num_feats, num_classes)
        model = model.to(device)
        init_ckpt_base = arch_config.get('init_checkpoint', None)
        if init_ckpt_base:
            ckpt_path = f"{init_ckpt_base}_{run}.pth"
            if os.path.exists(ckpt_path):
                ckpt = torch.load(ckpt_path, map_location=device)
                model.load_state_dict(ckpt['model_state_dict'])
                print(f"Loaded initial checkpoint for {model_name} run{run} from {ckpt_path}")
        run_config = config.copy()
        run_config['current_arch'] = model_name
        run_config['current_run'] = run
        run_config['class_map'] = class_map
        run_config['paths'] = {'models_dir': os.path.join("models", model_name)}
        os.makedirs(run_config['paths']['models_dir'], exist_ok=True)
        best_ckpts = train_model(model, train_loader, val_loader, run_config, device)
        saved_checkpoints.extend(best_ckpts)
    print("Training complete. Saved model checkpoints:")
    for ckpt in saved_checkpoints:
        print(" -", ckpt)


#### src/train_utils.py
# train_utils.py
import os
import yaml
import math
import numpy as np
import pandas as pd
import librosa
import cv2
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from sklearn.metrics import roc_auc_score

# --- Dataset definition ---
class BirdClefDataset(Dataset):
    """Dataset for BirdCLEF audio chunks with mel spectrograms and soft labels."""
    def __init__(self, metadata_df: pd.DataFrame, class_map: dict, 
                 mel_shape=(128, 640), augment=False):
        """
        metadata_df: DataFrame with columns: mel_path (or audio_path), label_json (or labels), weight, etc.
        class_map: dict mapping species code -> class index.
        mel_shape: expected shape (mel_bins, time_steps) of spectrogram (before channel expansion).
        augment: whether to apply data augmentation to mel spectrograms.
        """
        self.df = metadata_df.reset_index(drop=True)
        self.class_map = class_map
        self.num_classes = len(class_map)
        self.mel_shape = mel_shape
        self.augment = augment

        # Pre-extract label vectors and weights for speed
        self.labels = []
        self.sample_weights = []
        for _, row in self.df.iterrows():
            # If labels are stored as JSON string (e.g., {"sp1":0.7,"sp2":0.3}), parse it
            if 'label_json' in row and isinstance(row['label_json'], str):
                label_dict = json.loads(row['label_json'])
            else:
                # Otherwise, assume there's a column 'labels' as dict or similar
                label_dict = row.get('labels', {})
            # Initialize zero vector and fill in any present species
            label_vec = np.zeros(self.num_classes, dtype=np.float32)
            for sp, val in label_dict.items():
                if sp in self.class_map:  # only use species known in class_map
                    label_vec[self.class_map[sp]] = float(val)
            self.labels.append(label_vec)
            # Sample weight (for loss/sampling) - use 'weight' column if present, else default 1.0
            w = row.get('weight', 1.0)
            self.sample_weights.append(float(w))
        self.labels = np.array(self.labels, dtype=np.float32)
        self.sample_weights = np.array(self.sample_weights, dtype=np.float32)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Load precomputed mel spectrogram if available, otherwise compute from audio
        row = self.df.iloc[idx]
        # Determine source path
        if 'mel_path' in row and pd.notna(row['mel_path']):
            mel_path = row['mel_path']
            mel = np.load(mel_path)  # expecting shape (mel_bins, time_steps)
        else:
            # Compute mel from audio file path
            audio_path = row['audio_path'] if 'audio_path' in row else None
            if audio_path is None or not os.path.exists(audio_path):
                raise FileNotFoundError(f"No audio or mel file for index {idx}")
            # Load audio (mono) and compute mel spectrogram
            y, sr = librosa.load(audio_path, sr=self.sr)  # self.sr could be set from config, e.g., 32000
            # Pad or truncate to desired chunk duration
            target_len = int(self.chunk_duration * sr)
            if len(y) < target_len:
                # pad with silence (wrap around if needed for cyclic padding)
                pad_len = target_len - len(y)
                y = np.concatenate([y, np.zeros(pad_len, dtype=y.dtype)])
            else:
                y = y[:target_len]
            # Compute mel spectrogram
            mel = librosa.feature.melspectrogram(y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length, 
                                                n_mels=self.mel_shape[0], fmin=self.fmin, fmax=self.fmax)
            mel = librosa.power_to_db(mel, ref=np.max)  # convert to log-mel
            # Normalize mel (if desired), e.g., 0-1 scaling or standardization (not strictly necessary if model can learn scale)
            # mel = (mel - mel.mean()) / (mel.std() + 1e-6)
        # Ensure mel shape matches expected, resize if needed
        if mel.shape != tuple(self.mel_shape):
            # Resize spectrogram to target shape (e.g., 256x256) using interpolation
            mel = cv2.resize(mel, (self.mel_shape[1], self.mel_shape[0]))
        # Data augmentation on mel spectrogram (time/freq masking)
        if self.augment:
            mel = self._augment_mel(mel)
        # Convert to tensor and expand to 3-channel (for CNN input)
        mel_tensor = torch.from_numpy(mel).float()
        # Channel-first format for CNN (1 x H x W), then repeat to 3 x H x W to simulate RGB
        mel_tensor = mel_tensor.unsqueeze(0)  # shape: [1, H, W]
        mel_tensor = mel_tensor.repeat(3, 1, 1)  # shape: [3, H, W]
        # Prepare label and weight
        label_vec = torch.from_numpy(self.labels[idx])
        sample_w = torch.tensor(self.sample_weights[idx], dtype=torch.float32)
        return mel_tensor, label_vec, sample_w

    def _augment_mel(self, mel):
        """Apply in-place random time-frequency masking to the mel spectrogram (numpy array)."""
        H, W = mel.shape
        # Random frequency mask
        num_masks = np.random.randint(1, 3)  # e.g., 1 or 2 freq masks
        for _ in range(num_masks):
            f0 = np.random.randint(0, H)
            f_len = np.random.randint(5, H // 8)  # mask up to H/8 mel bins
            mel[f0:f0+f_len, :] = mel.mean()  # fill with mean or 0
        # Random time mask
        num_time_masks = np.random.randint(1, 3)
        for _ in range(num_time_masks):
            t0 = np.random.randint(0, W)
            t_len = np.random.randint(5, W // 8)  # mask up to W/8 time frames
            mel[:, t0:t0+t_len] = mel.mean()
        # (Additional augmentation like adding background noise, random shifts, mixup could be included as needed)
        return mel

# --- Sampler and DataLoader utility ---
def create_dataloader(dataset: BirdClefDataset, batch_size: int, shuffle: bool = False) -> DataLoader:
    """Create DataLoader for the given dataset, using weighted sampling if sample weights are provided."""
    if hasattr(dataset, 'sample_weights') and dataset.sample_weights is not None:
        # Use WeightedRandomSampler to sample each batch according to sample_weights
        sampler = WeightedRandomSampler(weights=dataset.sample_weights, num_samples=len(dataset), replacement=True)
        return DataLoader(dataset, batch_size=batch_size, sampler=sampler, 
                          num_workers=0 if os.name == 'nt' else 4, pin_memory=True)
    else:
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, 
                          num_workers=0 if os.name == 'nt' else 4, pin_memory=True)

# --- Training loop with validation ---
def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, 
                config: dict, device: torch.device):
    """
    Train the model for the specified number of epochs, evaluate on validation set, 
    and save checkpoints for top-3 validation scores.
    """
    epochs = config['training']['epochs']
    lr = config['optimizer']['lr']
    weight_decay = config['optimizer'].get('weight_decay', 0.0)
    optimizer_type = config['optimizer'].get('type', 'Adam')
    scheduler_cfg = config.get('scheduler', None)
    # Initialize optimizer (AdamW by default for better regularization)
    if optimizer_type.lower() == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    # Initialize LR scheduler if specified
    scheduler = None
    if scheduler_cfg:
        if scheduler_cfg['type'] == 'CosineAnnealingLR':
            t_max = scheduler_cfg.get('T_max', epochs)
            eta_min = scheduler_cfg.get('eta_min', 1e-6)
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)
        elif scheduler_cfg['type'] == 'StepLR':
            step_size = scheduler_cfg.get('step_size', 5)
            gamma = scheduler_cfg.get('gamma', 0.1)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
        # (Add other schedulers as needed)
    # Loss function: binary cross-entropy with logits (for multi-label classification)
    criterion = nn.BCEWithLogitsLoss(reduction='none')  # we'll handle reduction manually for weighting
    best_scores = []  # keep track of top validation scores (for model saving)
    best_checkpoints = []  # store corresponding state dicts or file paths

    for epoch in range(1, epochs+1):
        model.train()
        running_loss = 0.0
        for batch in train_loader:
            inputs, targets, sample_w = batch
            inputs = inputs.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)
            sample_w = sample_w.to(device, non_blocking=True)
            optimizer.zero_grad(set_to_none=True)
            # Forward pass
            logits = model(inputs)
            # Compute BCE loss per sample (and per class), then weight each sample's loss
            # criterion returns shape [batch, num_classes] since reduction='none'
            loss_matrix = criterion(logits, targets)
            # Average loss over classes for each sample, then apply sample weight
            # (target may be multi-label soft, but BCE is applied elementwise)
            loss_per_sample = loss_matrix.mean(dim=1)  # mean over classes
            weighted_loss = (loss_per_sample * sample_w)
            loss = weighted_loss.mean()  # average over batch
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        if scheduler:
            scheduler.step()
        avg_train_loss = running_loss / len(train_loader.dataset)

        # Validation phase at epoch end
        model.eval()
        all_preds = []
        all_targets = []
        with torch.no_grad():
            for batch in val_loader:
                inputs, targets, _ = batch  # no sample weight needed for evaluation
                inputs = inputs.to(device)
                targets = targets.to(device)
                logits = model(inputs)
                preds = torch.sigmoid(logits)  # convert to probabilities
                all_preds.append(preds.cpu().numpy())
                all_targets.append(targets.cpu().numpy())
        all_preds = np.vstack(all_preds)
        all_targets = np.vstack(all_targets)
        # Compute validation score (macro-AUC skipping classes with no positives)
        val_score = macro_auc_score(all_targets, all_preds)
        # (Optionally compute other metrics like macro-precision for monitoring)
        val_precision = macro_precision_score(all_targets, all_preds, threshold=0.5)
        print(f"Epoch {epoch}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val AUC: {val_score:.4f} - Val Prec@0.5: {val_precision:.4f}")

        # Save checkpoint if among top-3
        save_path = None
        if len(best_scores) < 3 or val_score > min(best_scores):
            # Save model state dict
            os.makedirs(config['paths']['models_dir'], exist_ok=True)
            arch_name = config['current_arch']  # set by training script
            run_id = config.get('current_run', 1)
            # Determine checkpoint file name
            ckpt_name = f"{arch_name}_run{run_id}_epoch{epoch}.pth"
            save_path = os.path.join(config['paths']['models_dir'], ckpt_name)
            torch.save({'model_state_dict': model.state_dict(), 
                        'species2idx': config['class_map']}, save_path)
            # Update best scores list
            if len(best_scores) < 3:
                best_scores.append(val_score)
                best_checkpoints.append(save_path)
            else:
                # replace worst of the top-3
                min_idx = np.argmin(best_scores)
                # Remove old worst checkpoint file if exists
                old_ckpt = best_checkpoints[min_idx]
                try:
                    os.remove(old_ckpt)
                except FileNotFoundError:
                    pass
                best_scores[min_idx] = val_score
                best_checkpoints[min_idx] = save_path
            print(f"Saved checkpoint: {save_path}")
    # End of training epochs
    # Return paths of best checkpoints
    # (Sorting by score so that best_scores[0] is highest)
    sorted_indices = np.argsort(best_scores)[::-1]
    best_checkpoints = [best_checkpoints[i] for i in sorted_indices]
    return best_checkpoints

# --- Evaluation metrics in training context (using functions from metrics.py, defined later) ---
def macro_auc_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Compute macro-averaged ROC-AUC, skipping classes with no positive true labels."""
    num_classes = y_true.shape[1]
    aucs = []
    for i in range(num_classes):
        # Only compute AUC if class has at least one positive in ground truth
        if np.sum(y_true[:, i]) == 0:
            continue
        try:
            auc = roc_auc_score(y_true[:, i], y_pred[:, i])
            aucs.append(auc)
        except ValueError:
            # This class might have all negatives in truth or constant predictions; skip it
            continue
    if len(aucs) == 0:
        return 0.0
    return float(np.mean(aucs))

def macro_precision_score(y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 0.5) -> float:
    """Compute macro-averaged precision at the given threshold."""
    num_classes = y_true.shape[1]
    precisions = []
    for i in range(num_classes):
        # Skip classes with no positive predictions and no positives in truth
        pred_positive = y_pred[:, i] >= threshold
        true_positive = y_true[:, i] == 1
        if pred_positive.sum() == 0:
            # If model predicts none for this class:
            if true_positive.sum() == 0:
                continue  # no instances of this class at all
            else:
                precisions.append(0.0)
                continue
        precision_i = np.sum(true_positive & pred_positive) / pred_positive.sum()
        precisions.append(precision_i)
    if len(precisions) == 0:
        return 0.0
    return float(np.mean(precisions))


#### src/utils.py
#!/usr/bin/env python3
"""
utils.py – Shared utility functions for the BirdCLEF 2025 pipeline.
Includes:
- Taxonomy loading and class mapping,
- Label vector creation for multi-label and pseudo-label samples,
- Chunk ID hashing for unique file naming,
- Mel spectrogram resizing.
"""
import ast
import hashlib
import numpy as np
from PIL import Image
import pandas as pd

def load_taxonomy(taxonomy_csv_path: str, train_csv_path: str):
    """
    Load species list and mapping from taxonomy or train data.
    Returns (class_list, class_map) where class_list is a list of species codes and 
    class_map is a dict mapping species code to index.
    """
    class_list = []
    if taxonomy_csv_path and pd.io.common.file_exists(taxonomy_csv_path):
        # If a taxonomy CSV is provided, use it to get the list of classes (species)
        tax_df = pd.read_csv(taxonomy_csv_path)
        # Assume there's a column with species code (could be 'ebird_code' or 'species_code' or similar)
        for col in ["ebird_code", "species_code", "primary_label"]:
            if col in tax_df.columns:
                class_list = list(pd.unique(tax_df[col]))
                break
    if not class_list:
        # Fallback: derive class list from training data
        train_df = pd.read_csv(train_csv_path)
        class_list = sorted(pd.unique(train_df["primary_label"]))
    # Ensure consistent ordering
    class_list = sorted(class_list)
    class_map = {species: idx for idx, species in enumerate(class_list)}
    return class_list, class_map

def create_label_vector(primary_label: str, secondary_labels: list, class_map: dict,
                        primary_weight: float = 0.7, secondary_weight: float = 0.3,
                        use_soft: bool = True) -> np.ndarray:
    """
    Create a label vector for a training chunk.
    - If use_soft is True and secondary_labels is non-empty, assign primary_label a weight (primary_weight)
      and distribute secondary_weight among secondary_labels (evenly if multiple).
    - If use_soft is False or there are no secondary labels, returns a one-hot vector for primary and any secondaries (multi-hot if multiple and not soft).
    """
    num_classes = len(class_map)
    label_vec = np.zeros(num_classes, dtype=np.float32)
    if not secondary_labels or not use_soft:
        # No secondaries or we want hard labels
        # Mark primary as 1
        if primary_label in class_map:
            label_vec[class_map[primary_label]] = 1.0
        # If there are secondary labels but no soft labeling, mark them as well (multi-hot)
        if secondary_labels and not use_soft:
            for sec in secondary_labels:
                if sec in class_map:
                    label_vec[class_map[sec]] = 1.0
    else:
        # Soft label distribution: primary and secondaries
        if primary_label in class_map:
            label_vec[class_map[primary_label]] = primary_weight
        if secondary_labels:
            # distribute secondary_weight among all secondary labels
            share = secondary_weight
            sec_count = len(secondary_labels)
            if sec_count > 0:
                per_sec = share / sec_count
            else:
                per_sec = 0.0
            for sec in secondary_labels:
                if sec in class_map:
                    label_vec[class_map[sec]] = per_sec
    return label_vec

def hash_chunk_id(filename: str, start_sec: float) -> str:
    """
    Generate a unique hash ID for a chunk given the source filename and start time.
    """
    base = f"{filename}_{start_sec:.3f}"
    # Use a short hash for uniqueness
    h = hashlib.sha1(base.encode('utf-8')).hexdigest()
    return h[:8]  # use first 8 hex digits for brevity

def resize_mel(mel_db: np.ndarray, target_h: int, target_w: int) -> np.ndarray:
    """
    Resize a mel spectrogram (in dB) to the target height and width using bilinear interpolation.
    Preserves dynamic range by normalizing before resize and re-scaling after.
    """
    h, w = mel_db.shape
    if (h, w) == (target_h, target_w):
        return mel_db
    mel_min, mel_max = mel_db.min(), mel_db.max()
    # Normalize to 0-255
    mel_norm = (mel_db - mel_min) / (mel_max - mel_min + 1e-6)
    mel_img = Image.fromarray((mel_norm * 255).astype(np.uint8))
    mel_img = mel_img.resize((target_w, target_h), Image.BILINEAR)
    mel_resized = np.array(mel_img).astype(np.float32) / 255.0
    # Re-scale to original dB range
    mel_resized = mel_resized * (mel_max - mel_min + 1e-6) + mel_min
    return mel_resized


