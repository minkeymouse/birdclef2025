#### .pytest_cache/README.md
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


#### code_collector.py
#!/usr/bin/env python3
"""
collect_code.py – concatenate code files into a single text file

Walks the current directory and all subdirectories, finds files with common
code extensions, and writes their contents (with headers) into one output file.

Usage:
  python collect_code.py [--output ALL_CODE.txt] [--extensions .py,.js,...]
"""

import os
import argparse
from pathlib import Path

# Default extensions to include
DEFAULT_EXTS = [
    '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.hpp', '.sh', '.bat', '.yaml', '.yml', '.json', '.md'
]


def collect_code_files(root: Path, extensions: list[str]) -> list[Path]:
    """Recursively collect files under root matching the given extensions."""
    matches = []
    for dirpath, _, filenames in os.walk(root):
        for fname in filenames:
            if any(fname.endswith(ext) for ext in extensions):
                matches.append(Path(dirpath) / fname)
    return matches


def concatenate_files(files: list[Path], output: Path) -> None:
    """Write each file's path and contents into the output file."""
    with output.open('w', encoding='utf-8') as out_f:
        for fpath in sorted(files):
            out_f.write(f"#### {fpath}\n")
            try:
                text = fpath.read_text(encoding='utf-8')
            except Exception:
                # Binary or unreadable file
                out_f.write(f"[Could not read contents of {fpath}]\n\n")
                continue
            out_f.write(text)
            out_f.write("\n\n")
    print(f"Collected {len(files)} files into {output}")


def parse_args():
    p = argparse.ArgumentParser(description="Concatenate code files into one text file.")
    p.add_argument(
        '--output', '-o', type=Path, default=Path('ALL_CODE.txt'),
        help='Output file path'
    )
    p.add_argument(
        '--extensions', '-e', type=lambda s: s.split(','),
        default=DEFAULT_EXTS,
        help='Comma-separated list of file extensions to include'
    )
    p.add_argument(
        '--root', '-r', type=Path, default=Path('.'),
        help='Root directory to search'
    )
    return p.parse_args()


def main():
    args = parse_args()
    files = collect_code_files(args.root, args.extensions)
    concatenate_files(files, args.output)


if __name__ == '__main__':
    main()


#### configure.py
# configure.py – central configuration for the **BirdCLEF 2025** pipeline
# =============================================================================
# One authoritative source of truth for every tunable used by the codebase.
# Edit paths & hyper‑parameters here, re‑run the affected stage, and *never*
# hunt for magic numbers scattered across modules again.
# =============================================================================

from __future__ import annotations

from pathlib import Path
from typing import List, Dict

class CFG:  # pylint: disable=too-few-public-methods
    """Global, immutable configuration namespace."""
    # ────────────────────────────────────────────────────────────────────────
    # Reproducibility & runtime
    # ────────────────────────────────────────────────────────────────────────
    SEED: int = 42              # master RNG seed
    DEVICE: str = "cuda"         # "cuda" | "gpu" | "cpu"
    # ────────────────────────────────────────────────────────────────────────
    # Filesystem layout  – adjust to your environment / Kaggle dataset mount
    # ────────────────────────────────────────────────────────────────────────
    DATA_ROOT: Path = Path("/data/birdclef")

    TRAIN_AUDIO_DIR: Path = DATA_ROOT / "train_audio"
    TRAIN_SOUNDSCAPE_DIR: Path = DATA_ROOT / "train_soundscapes"
    TRAIN_CSV: Path = DATA_ROOT / "train.csv"
    TAXONOMY_CSV: Path = DATA_ROOT / "taxonomy.csv"
    TEST_DIR: Path = DATA_ROOT / "test_soundscapes/"
    SAMPLE_SUBMISSION: Path = DATA_ROOT / "sample_submission.csv"

    # Pre‑processing outputs
    PROCESSED_DIR: Path = DATA_ROOT / "processed"  # mels with labels for training data

    # Model checkpoints
    MODELS_DIR: Path = DATA_ROOT / "models"

    # Inference artifact
    SUBMISSION_OUT: Path = DATA_ROOT / "submission.csv"

    # ────────────────────────────────────────────────────────────────────────
    # Audio / spectrogram params – **keep consistent across modules**
    # ────────────────────────────────────────────────────────────────────────
    SAMPLE_RATE: int = 32_000
    N_FFT: int = 1024
    HOP_LENGTH: int = 500
    N_MELS: int = 128
    FMIN: int = 40
    FMAX: int = 15_000
    POWER: float = 2.0

    TARGET_SHAPE: tuple[int, int] = (256, 256)  # resize for CNN

    # ────────────────────────────────────────────────────────────────────────
    # Chunking strategy
    # ────────────────────────────────────────────────────────────────────────
    TRAIN_CHUNK_SEC: int = 10
    TRAIN_CHUNK_HOP_SEC: int = 5
    SC_SEG_SEC: int = 5  # evaluation granularity
    FOLD0_RATIO: float = 0.80  # % of cleanest clips used for training

    # ────────────────────────────────────────────────────────────────────────
    # Pre‑processing heuristics
    # ────────────────────────────────────────────────────────────────────────
    TRIM_TOP_DB: int = 20
    RMS_THRESHOLD: float = 0.01
    MIN_RATING: int = 1
    PSEUDO_THRESHOLD: float = 0.50

    USE_SOFT_LABELS: bool = True
    LABEL_WEIGHT_PRIMARY: float = 0.7
    LABEL_WEIGHT_SECONDARY: float = 0.2
    LABEL_WEIGHT_BENCH: float = 0.1

    RARE_COUNT_THRESHOLD: int = 20
    RARE_WEIGHT: float = 2.0
    PSEUDO_WEIGHT: float = 0.5

    MEL_CACHE_SIZE: int = 2048

    # ────────────────────────────────────────────────────────────────────────
    # Data‑augmentation
    # ────────────────────────────────────────────────────────────────────────
    SPEC_AUG_FREQ_MASK_PARAM: int = 10
    SPEC_AUG_TIME_MASK_PARAM: int = 50
    SPEC_AUG_NUM_MASKS: int = 2
    CUTMIX_PROB: float = 0.5

    # ────────────────────────────────────────────────────────────────────────
    # EfficientNet‑B0
    # ────────────────────────────────────────────────────────────────────────
    EFF_NUM_MODELS: int = 2
    EFF_BATCH_SIZE: int = 32
    EFF_EPOCHS: int = 10
    EFF_LR: float = 2e-3
    EFF_WEIGHT_DECAY: float = 1e-4
    EFF_NUM_WORKERS: int = 4

    # ────────────────────────────────────────────────────────────────────────
    # RegNetY‑0.8GF
    # ────────────────────────────────────────────────────────────────────────
    REG_NUM_MODELS: int = 2
    REG_BATCH_SIZE: int = 32
    REG_EPOCHS: int = 10
    REG_LR: float = 2e-3
    REG_WEIGHT_DECAY: float = 1e-4
    REG_NUM_WORKERS: int = 4

    # ────────────────────────────────────────────────────────────────────────
    # DiffWave minority‑class synthesis
    # ────────────────────────────────────────────────────────────────────────
    DIFF_RARE_THRESHOLD: int = 10  # if real recordings < 10 ⇒ target for synth

    # ────────────────────────────────────────────────────────────────────────
    # Inference / ensemble parameters
    # ────────────────────────────────────────────────────────────────────────
    INF_BATCH_SIZE: int = 16
    INF_NUM_WORKERS: int = 4
    INF_SMOOTH_NEIGHBORS: int = 2  # ±2 × 5 s segments

    # ────────────────────────────────────────────────────────────────────────
    # CPU optimisation (OpenVINO / ONNX)
    # ────────────────────────────────────────────────────────────────────────
    USE_OPENVINO: bool = True
    OV_NUM_THREADS: int | None = None

    # ────────────────────────────────────────────────────────────────────────
    # Class mapping
    # ────────────────────────────────────────────────────────────────────────
    SPECIES: Dict[int, str] = {
        0: '1139490',
        1: '1192948',
        2: '1194042',
        3: '126247',
        4: '1346504',
        5: '134933',
        6: '135045',
        7: '1462711',
        8: '1462737',
        9: '1564122',
        10: '21038',
        11: '21116',
        12: '21211',
        13: '22333',
        14: '22973',
        15: '22976',
        16: '24272',
        17: '24292',
        18: '24322',
        19: '41663',
        20: '41778',
        21: '41970',
        22: '42007',
        23: '42087',
        24: '42113',
        25: '46010',
        26: '47067',
        27: '476537',
        28: '476538',
        29: '48124',
        30: '50186',
        31: '517119',
        32: '523060',
        33: '528041',
        34: '52884',
        35: '548639',
        36: '555086',
        37: '555142',
        38: '566513',
        39: '64862',
        40: '65336',
        41: '65344',
        42: '65349',
        43: '65373',
        44: '65419',
        45: '65448',
        46: '65547',
        47: '65962',
        48: '66016',
        49: '66531',
        50: '66578',
        51: '66893',
        52: '67082',
        53: '67252',
        54: '714022',
        55: '715170',
        56: '787625',
        57: '81930',
        58: '868458',
        59: '963335',
        60: 'amakin1',
        61: 'amekes',
        62: 'ampkin1',
        63: 'anhing',
        64: 'babwar',
        65: 'bafibi1',
        66: 'banana',
        67: 'baymac',
        68: 'bbwduc',
        69: 'bicwre1',
        70: 'bkcdon',
        71: 'bkmtou1',
        72: 'blbgra1',
        73: 'blbwre1',
        74: 'blcant4',
        75: 'blchaw1',
        76: 'blcjay1',
        77: 'blctit1',
        78: 'blhpar1',
        79: 'blkvul',
        80: 'bobfly1',
        81: 'bobher1',
        82: 'brtpar1',
        83: 'bubcur1',
        84: 'bubwre1',
        85: 'bucmot3',
        86: 'bugtan',
        87: 'butsal1',
        88: 'cargra1',
        89: 'cattyr',
        90: 'chbant1',
        91: 'chfmac1',
        92: 'cinbec1',
        93: 'cocher1',
        94: 'cocwoo1',
        95: 'colara1',
        96: 'colcha1',
        97: 'compau',
        98: 'compot1',
        99: 'cotfly1',
        100: 'crbtan1',
        101: 'crcwoo1',
        102: 'crebob1',
        103: 'cregua1',
        104: 'creoro1',
        105: 'eardov1',
        106: 'fotfly',
        107: 'gohman1',
        108: 'grasal4',
        109: 'grbhaw1',
        110: 'greani1',
        111: 'greegr',
        112: 'greibi1',
        113: 'grekis',
        114: 'grepot1',
        115: 'gretin1',
        116: 'grnkin',
        117: 'grysee1',
        118: 'gybmar',
        119: 'gycwor1',
        120: 'labter1',
        121: 'laufal1',
        122: 'leagre',
        123: 'linwoo1',
        124: 'littin1',
        125: 'mastit1',
        126: 'neocor',
        127: 'norscr1',
        128: 'olipic1',
        129: 'orcpar',
        130: 'palhor2',
        131: 'paltan1',
        132: 'pavpig2',
        133: 'piepuf1',
        134: 'pirfly1',
        135: 'piwtyr1',
        136: 'plbwoo1',
        137: 'plctan1',
        138: 'plukit1',
        139: 'purgal2',
        140: 'ragmac1',
        141: 'rebbla1',
        142: 'recwoo1',
        143: 'rinkin1',
        144: 'roahaw',
        145: 'rosspo1',
        146: 'royfly1',
        147: 'rtlhum',
        148: 'rubsee1',
        149: 'rufmot1',
        150: 'rugdov',
        151: 'rumfly1',
        152: 'ruther1',
        153: 'rutjac1',
        154: 'rutpuf1',
        155: 'saffin',
        156: 'sahpar1',
        157: 'savhaw1',
        158: 'secfly1',
        159: 'shghum1',
        160: 'shtfly1',
        161: 'smbani',
        162: 'snoegr',
        163: 'sobtyr1',
        164: 'socfly1',
        165: 'solsan',
        166: 'soulap1',
        167: 'spbwoo1',
        168: 'speowl1',
        169: 'spepar1',
        170: 'srwswa1',
        171: 'stbwoo2',
        172: 'strcuc1',
        173: 'strfly1',
        174: 'strher',
        175: 'strowl1',
        176: 'tbsfin1',
        177: 'thbeup1',
        178: 'thlsch3',
        179: 'trokin',
        180: 'tropar',
        181: 'trsowl',
        182: 'turvul',
        183: 'verfly',
        184: 'watjac1',
        185: 'wbwwre1',
        186: 'whbant1',
        187: 'whbman1',
        188: 'whfant1',
        189: 'whmtyr1',
        190: 'whtdov',
        191: 'whttro1',
        192: 'whwswa1',
        193: 'woosto',
        194: 'y00678',
        195: 'yebela1',
        196: 'yebfly1',
        197: 'yebsee1',
        198: 'yecspi2',
        199: 'yectyr1',
        200: 'yehbla2',
        201: 'yehcar1',
        202: 'yelori1',
        203: 'yeofly1',
        204: 'yercac1',
        205: 'ywcpar'}

# EOF

#### data_utils.py
#!/usr/bin/env python
"""data_utils.py – shared utility layer for the **BirdCLEF 2025** solution
====================================================================
This module centralises *all* common data-handling functionality so that the
rest of the pipeline (``process.py``, ``efficientnet.py``, ``regnety.py``,
``diffwave.py``) can remain lean.  Key capabilities:

* Audio I/O with deterministic resampling (+ optional WebRTC VAD removal)
* Mel-spectrogram extraction via librosa
* SpecAugment + CutMix implementations fully driven by ``configure.CFG``
* Soft-label aware `torch.utils.data.Dataset` (`MelDataset`)
* One-chunk-per-file sampling (`FileWiseSampler`) to match training recipe
* LRU-cached on-disk mel loader to save repeated numpy I/O
* Noise metric helper used by ``process.py`` for fold-0 split

The API surface is intentionally minimal – *import and call what you need*.
"""
from __future__ import annotations

import json
import os
import random
from functools import lru_cache
from pathlib import Path
from typing import Dict, Iterable, List, Tuple, Union, Optional
import webrtcvad

import cv2
import librosa
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, Sampler

from configure import CFG

__all__ = [
    "seed_everything",
    "compute_noise_metric",
    "load_audio",
    "trim_silence",
    "load_vad",
    "remove_speech",
    "compute_mel",
    "segment_audio",
    "spec_augment",
    "cutmix",
    "MelDataset",
]

# ────────────────────────────────────────────────────────────────────────
# Reproducibility helpers
# ────────────────────────────────────────────────────────────────────────

def seed_everything(seed: int = 42) -> None:
    """Seed every RNG we know about for deterministic runs."""
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# ────────────────────────────────────────────────────────────────────────
# Augmentations – SpecAugment & CutMix
# ────────────────────────────────────────────────────────────────────────

def spec_augment(
    mel: np.ndarray,
    *,
    freq_mask_param: Optional[int] = None,
    time_mask_param: Optional[int] = None,
    num_masks: Optional[int] = None,
) -> np.ndarray:
    """Apply SpecAugment (frequency & time masking) on a mel spectrogram."""
    fmp = freq_mask_param if freq_mask_param is not None else CFG.SPEC_AUG_FREQ_MASK_PARAM
    tmp = time_mask_param if time_mask_param is not None else CFG.SPEC_AUG_TIME_MASK_PARAM
    nmk = num_masks if num_masks is not None else CFG.SPEC_AUG_NUM_MASKS

    H, W = mel.shape
    out = mel.copy()
    for _ in range(nmk):
        if fmp > 0:
            fh = np.random.randint(0, fmp + 1)
            f0 = np.random.randint(0, max(1, H - fh)) if fh else 0
            out[f0 : f0 + fh, :] = 0.0
        if tmp > 0:
            th = np.random.randint(0, tmp + 1)
            t0 = np.random.randint(0, max(1, W - th)) if th else 0
            out[:, t0 : t0 + th] = 0.0
    return out


def cutmix(
    m1: np.ndarray,
    l1: torch.Tensor,
    m2: np.ndarray,
    l2: torch.Tensor,
) -> Tuple[np.ndarray, torch.Tensor]:
    """Horizontal CutMix for mels; label = length-weighted interpolation."""
    W = m1.shape[1]
    if W < 2:
        return m1, l1
    cut = np.random.randint(1, W)
    mixed = np.concatenate([m1[:, :cut], m2[:, cut:]], axis=1)
    alpha = cut / W
    label = l1 * alpha + l2 * (1.0 - alpha)
    return mixed, label

# ────────────────────────────────────────────────────────────────────────
# Audio helpers (load / trim / VAD)
# ────────────────────────────────────────────────────────────────────────

def load_audio(
    fp: Union[Path, str],
    sample_rate: Optional[int] = None,
    *,
    return_sr: bool = False,
) -> Union[np.ndarray, Tuple[np.ndarray, int]]:
    """Load **mono** audio as 32-bit float @ ``sample_rate`` (default CFG)."""
    sr = sample_rate or CFG.SAMPLE_RATE
    y, _ = librosa.load(str(fp), sr=sr, mono=True)
    y = y.astype(np.float32)
    return (y, sr) if return_sr else y


def trim_silence(y: np.ndarray) -> np.ndarray:
    """Energy-based leading / trailing trim."""
    y_trim, _ = librosa.effects.trim(y, top_db=CFG.TRIM_TOP_DB)
    return y_trim


def compute_noise_metric(y: np.ndarray) -> float:
    """Composite "noise" metric – smaller ⇒ cleaner recording."""
    return float(y.std() + y.var() + np.sqrt((y ** 2).mean()) + (y ** 2).sum())


def load_vad():
    """Return a *WebRTC VAD* instance & helper TS-function or ``(None, None)``."""
    vad = webrtcvad.Vad(3)
    def _iter_frames(wav: np.ndarray, sr: int, frame_ms: int = 30):
        flen = int(sr * frame_ms / 1000)
        for i in range(0, len(wav) - flen, flen):
            yield wav[i : i + flen]
    def _get_ts(wav: np.ndarray, sr: int):
        voiced: List[Tuple[int, int]] = []
        flen = int(sr * 0.03)
        in_voiced = False
        start = 0
        for idx, frame in enumerate(_iter_frames(wav, sr)):
            speech = vad.is_speech((frame * 32768).astype("int16").tobytes(), sr)
            if speech and not in_voiced:
                start = idx * flen
                in_voiced = True
            elif not speech and in_voiced:
                voiced.append((start, idx * flen))
                in_voiced = False
        if in_voiced:
            voiced.append((start, len(wav)))
        return voiced
    return vad, _get_ts

def remove_speech(y: np.ndarray, vad_model, get_ts):
    """Zero-out VAD-detected speech regions (if VAD available)."""
    if vad_model is None:
        return y
    mask = np.ones_like(y, dtype=bool)
    for s, e in get_ts(y, CFG.SAMPLE_RATE):
        mask[s:e] = False
    return y[mask]

# ────────────────────────────────────────────────────────────────────────
# Mel-spectrogram extraction via Librosa only
# ────────────────────────────────────────────────────────────────────────

def compute_mel(y: np.ndarray, *, to_db: bool = True) -> np.ndarray:
    """Return *N_MELS × T* mel spectrogram in linear or dB scale using Librosa."""
    mel = librosa.feature.melspectrogram(
        y=y,
        sr=CFG.SAMPLE_RATE,
        n_fft=CFG.N_FFT,
        hop_length=CFG.HOP_LENGTH,
        n_mels=CFG.N_MELS,
        fmin=CFG.FMIN,
        fmax=CFG.FMAX,
        power=CFG.POWER,
    )
    if to_db:
        mel = librosa.power_to_db(mel, ref=np.max)
    return mel.astype(np.float32)

# ────────────────────────────────────────────────────────────────────────
# Segmentation helper – yields fixed-length chunks (wrap-pad)
# ────────────────────────────────────────────────────────────────────────

def segment_audio(
    y: np.ndarray,
    *,
    chunk_sec: Optional[float] = None,
    hop_sec: Optional[float] = None,
    sr: Optional[int] = None,
) -> Iterable[Tuple[float, np.ndarray]]:
    chunk_sec = chunk_sec if chunk_sec is not None else CFG.TRAIN_CHUNK_SEC
    hop_sec = hop_sec if hop_sec is not None else CFG.TRAIN_CHUNK_HOP_SEC
    sr = sr if sr is not None else CFG.SAMPLE_RATE
    chunk_len = int(chunk_sec * sr)
    hop_len = int(hop_sec * sr)
    n = len(y)
    for start in range(0, n, hop_len):
        chunk = y[start : start + chunk_len]
        if len(chunk) < chunk_len:
            chunk = np.pad(chunk, (0, chunk_len - len(chunk)), mode="wrap")
        yield start / sr, chunk

# ────────────────────────────────────────────────────────────────────────
# Dataset + Sampler for training
# ────────────────────────────────────────────────────────────────────────

class #SOME SAMPLER HERE#(Sampler[int]):


class MelDataset(Dataset):
    def __init__(
        self,
        cls_ind: Dict = CFG.SPECIES,
        *,
        augment: bool = False,
        labeled: bool = False,
        pseudo_label: bool = False,
    ):
        self.cls_ind = cls_ind
        self.augment = augment
        self.labeled = labeled
        self.pseudo_label = pseudo_label

    def _load_norm(self, rel: str) -> np.ndarray:
        full = CFG.PROCESSED_DIR / rel
        mel = np.load(full).astype(np.float32)
        mel = (mel - mel.min()) / (mel.max() - mel.min() + 1e-6)
        return cv2.resize(mel, CFG.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)

    def _json_to_vec(self, js: str) -> torch.Tensor:
        vec = np.zeros(len(self.s2i), dtype=np.float32)
        for sp, w in json.loads(js).items():
            idx = self.s2i.get(sp)
            if idx is not None:
                vec[idx] = w
        return torch.from_numpy(vec)

    def __len__(self) -> int:
        return len(self.df)

    def __getitem__(self, idx: int):
        row = self.df.iloc[idx]
        mel = self._load_norm(row["mel_path"])
        if self.augment:
            mel = spec_augment(mel)
        if self.labeled and not self.pseudo_label:
            if row.get("label_path", "").endswith(".npy"):
                vec = np.load(CFG.PROCESSED_DIR / row["label_path"], allow_pickle=False)
                label_vec = torch.from_numpy(vec.astype(np.float32))
            else:
                label_vec = self._json_to_vec(row["label_json"])
        elif self.labeled and self.pseudo_label:
            raise NotImplementedError("Cannot pseudo label already labeled data")
        elif not self.labeled and self.pseudo_label:


        mel_tensor = torch.tensor(mel).unsqueeze(0)
        if self.augment and random.random() < CFG.CUTMIX_PROB and len(self.df) > 1:
            j = random.randrange(len(self.df))
            if j == idx:
                j = (j + 1) % len(self.df)
            row2 = self.df.iloc[j]
            mel2 = spec_augment(self._load_norm(row2["mel_path"])) if self.augment else self._load_norm(row2["mel_path"])
            mel_mix, label_vec = cutmix(mel_tensor.squeeze(0).numpy(), label_vec, mel2, label_vec2)
            mel_tensor = torch.tensor(mel_mix).unsqueeze(0)
        weight = float(row.get("weight", 1.0))
        return mel_tensor, label_vec, weight


#### diffwave.py
#!/usr/bin/env python3
"""
diffwave.py – generate synthetic audio for BirdCLEF 2025
======================================================

Uses a pretrained SpeechBrain DiffWave vocoder to convert precomputed
mel-spectrogram chunks into synthetic .ogg clips for rare species.
Also supports cleanup of synthetic files and patching train.csv.

Usage:
  python diffwave.py generate [--species S1,S2]
  python diffwave.py remove
"""
from __future__ import annotations
import argparse
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import soundfile as sf
from speechbrain.inference.vocoders import DiffWaveVocoder
from configure import CFG

# ────────────────────────────────────────────────────────────
# Config & Hyperparameters
# ────────────────────────────────────────────────────────────
SAMPLE_RATE = CFG.SAMPLE_RATE  # 32000
# Thresholds for deciding generation plan
THRESH_LOW, THRESH_HIGH = 20, 50
TARGET_LOW, TARGET_MID = 20, 5

# ────────────────────────────────────────────────────────────
# Helpers
# ────────────────────────────────────────────────────────────

def compute_plan(csv_path: Path) -> Dict[str, int]:
    """Decide how many synthetic clips per species based on counts."""
    df = pd.read_csv(csv_path)
    counts = df["primary_label"].value_counts()
    plan: Dict[str, int] = {}
    for sp, cnt in counts.items():
        # cast to string key for paths
        sp_key = str(sp)
        if cnt < THRESH_LOW:
            plan[sp_key] = max(0, TARGET_LOW - int(cnt))
        elif cnt < THRESH_HIGH:
            plan[sp_key] = TARGET_MID
        else:
            plan[sp_key] = 0
    return plan


def patch_train_csv(rows: List[Tuple[str, str]]) -> None:
    """Append new synthetic entries to train.csv, avoiding duplicates."""
    df = pd.read_csv(CFG.TRAIN_CSV)
    exist = set(zip(df["primary_label"], df["filename"]))
    new = [r for r in rows if r not in exist]
    if not new:
        return
    extra = pd.DataFrame(new, columns=["primary_label", "filename"])
    for col in df.columns:
        if col not in extra.columns:
            extra[col] = pd.NA
    out = pd.concat([df, extra[df.columns]], ignore_index=True)
    out.to_csv(CFG.TRAIN_CSV, index=False)
    print(f"Appended {len(new)} synthetic entries to {CFG.TRAIN_CSV}")


def generate(args: argparse.Namespace) -> None:
    """Generate synthetic .ogg files from mel-spectrogram arrays."""
    device = "cuda" if CFG.use_cuda() else "cpu"
    vb = DiffWaveVocoder.from_hparams(
        source="speechbrain/tts-diffwave-ljspeech",
        savedir=CFG.DIFFWAVE_MODEL_DIR / "vocoder",
        run_opts={"device": device},
    )
    plan = compute_plan(CFG.TRAIN_CSV)
    rows: List[Tuple[str, str]] = []
    base = CFG.PROCESSED_DIR / "mels" / "train"

    for sp, n in plan.items():
        if n <= 0 or (args.species and sp not in args.species):
            continue
        # ensure species as string for path operations
        sp_str = str(sp)
        sp_dir = base / sp_str
        if not sp_dir.exists():
            continue
        mel_files = sorted(sp_dir.glob("*.npy"))
        count = 0
        for mel_fp in mel_files:
            if count >= n:
                break
            mel_np = np.load(mel_fp)
            mel_tensor = torch.from_numpy(mel_np).unsqueeze(0).to(device)
            wav = vb.decode_spectrogram(
                mel_tensor,
                hop_length=CFG.HOP_LENGTH,
                fast_sampling=True,
                fast_sampling_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],
            )  # [1, time_steps]
            out_fn = f"synthetic_{count:03d}.ogg"
            out_fp = CFG.TRAIN_AUDIO_DIR / sp / out_fn
            out_fp.parent.mkdir(parents=True, exist_ok=True)
            # create an empty placeholder file instead of encoding
            out_fp.write_bytes(b"")
            rows.append((sp, out_fn))
            count += 1

    if rows:
        patch_train_csv(rows)
    else:
        print("No synthetic clips generated.")


def remove(args: argparse.Namespace) -> None:
    """Delete all synthetic .ogg files."""
    for sp_dir in CFG.TRAIN_AUDIO_DIR.iterdir():
        if sp_dir.is_dir():
            for f in sp_dir.glob("synthetic_*.ogg"):
                f.unlink(missing_ok=True)
    print("Removed all synthetic audio files.")


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(prog="diffwave.py")
    sub = p.add_subparsers(dest="cmd", required=True)

    gen = sub.add_parser("generate", help="Generate synthetic audio from mel files")
    gen.add_argument(
        "--species", type=lambda s: s.split(","),
        help="Comma-separated list of species to target",
    )

    sub.add_parser("remove", help="Remove synthetic audio files")
    return p.parse_args()


def main() -> None:
    args = parse_args()
    if args.cmd == "generate":
        generate(args)
    elif args.cmd == "remove":
        remove(args)

if __name__ == "__main__":
    main()


#### efficientnet.py
#!/usr/bin/env python3
"""
efficientnet.py – EfficientNet‑B0 ensemble trainer for **BirdCLEF 2025**
=====================================================================
Trains *CFG.EFF_NUM_MODELS* EfficientNet‑B0 classifiers on the mel‑spectrogram
chunks produced by ``process.py``.  The script follows the 2024 winning recipe
and the workflow requested by the user:

* Cross‑entropy with **soft‑label** support
* One‑chunk‑per‑file sampling via ``FileWiseSampler``
* SpecAugment + CutMix enabled through *data_utils*
* Cosine‑annealing LR schedule & mixed‑precision (AMP)
* Optional device override via ``--device`` (auto / cpu / gpu)
* Reproducible ensemble – each run uses a different seed offset
* Checkpoints saved under ``CFG.EFF_MODEL_DIR`` as
  ``efficientnet_b0_run{RUN}.pth`` (state + species mapping)

Usage
-----
```bash
# train two models on GPU (default)
python efficientnet.py              #   → models/efficientnet/*.pth

# force CPU training & 5 epochs only (quick test)
python efficientnet.py --device cpu --epochs 5
```
"""
from __future__ import annotations

import argparse
import json
import logging
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import timm
from torch import optim
from torch.cuda.amp import GradScaler, autocast
from torch.utils.data import DataLoader

from configure import CFG
from data_utils import FileWiseSampler, MelDataset, seed_everything

# ───────────────────────── logging ──────────────────────────

def _logger() -> logging.Logger:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.StreamHandler()],
    )
    return logging.getLogger("efficientnet")


# ──────────────────── soft‑label CE helper ───────────────────

def _soft_ce(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    """Cross‑entropy that supports *either* hard indices *or* soft vectors."""
    if targets.dtype == torch.long:
        return F.cross_entropy(logits, targets, reduction="none")
    logp = torch.log_softmax(logits, dim=1)
    return -(targets * logp).sum(dim=1)


# ───────────────────────── trainer ───────────────────────────

class _Trainer:
    """Encapsulates one EfficientNet run (different seed / init)."""

    def __init__(self, run_id: int, classes: List[str], log: logging.Logger):
        self.run_id, self.log = run_id, log
        self.device = torch.device("cuda" if CFG.use_cuda() else "cpu")
        self.classes = classes

        # model – EfficientNet‑B0  (1‑channel input)
        self.model = timm.create_model(
            "efficientnet_b0",
            pretrained=True,
            in_chans=1,
            num_classes=len(classes),
        ).to(self.device)

        self.opt = optim.AdamW(
            self.model.parameters(),
            lr=CFG.EFF_LR,
            weight_decay=CFG.EFF_WEIGHT_DECAY,
        )
        self.sched = optim.lr_scheduler.CosineAnnealingLR(
            self.opt, T_max=CFG.EFF_EPOCHS
        )
        self.scaler = GradScaler(enabled=CFG.use_cuda())

    # ‑‑ one optimisation step ‑─────────────────────────────
    def _step(self, batch):
        x, y, w = (t.to(self.device, non_blocking=True) for t in batch)
        self.opt.zero_grad(set_to_none=True)
        with autocast(enabled=CFG.use_cuda()):
            logits = self.model(x)
            loss = (_soft_ce(logits, y) * w).mean()
        self.scaler.scale(loss).backward()
        self.scaler.step(self.opt)
        self.scaler.update()
        return float(loss.detach())

    # ‑‑ epoch loop ‑─────────────────────────────────────────
    def fit(self, loader: DataLoader) -> None:
        for ep in range(1, self._epochs + 1):
            self.model.train(); run_loss = 0.0
            for batch in loader:
                run_loss += self._step(batch) * batch[0].size(0)
            self.sched.step()
            self.log.info(
                "run=%d  epoch=%d/%d  loss=%.5f  lr=%.2e",
                self.run_id,
                ep,
                self._epochs,
                run_loss / len(loader.dataset),
                self.sched.get_last_lr()[0],
            )

    # property wrapper to cope with CLI‑overridden epochs
    @property
    def _epochs(self) -> int:
        return getattr(CFG, "EFF_EPOCHS", 10)

    # ‑‑ save checkpoint ‑────────────────────────────────────
    def save(self, out_dir: Path) -> None:
        out_dir.mkdir(parents=True, exist_ok=True)
        ckpt = {
            "arch": "efficientnet_b0",
            "model": self.model.state_dict(),
            "species2idx": {s: i for i, s in enumerate(self.classes)},
        }
        fp = out_dir / f"efficientnet_b0_run{self.run_id}.pth"
        torch.save(ckpt, fp)
        self.log.info("✔ saved checkpoint → %s", fp.name)


# ─────────────────── metadata & DataLoader ───────────────────

def _load_meta() -> pd.DataFrame:
    df = pd.read_csv(CFG.PROCESSED_DIR / "train_metadata.csv")
    sc = CFG.PROCESSED_DIR / "soundscape_metadata.csv"
    if sc.exists():
        df = pd.concat([df, pd.read_csv(sc)], ignore_index=True)
    return df


# ─────────────────────────── main ────────────────────────────

def main() -> None:  # noqa: D401
    cli = argparse.ArgumentParser(description="Train EfficientNet‑B0 ensemble")
    cli.add_argument("--device", choices=["auto", "cpu", "gpu", "cuda"], default="auto")
    cli.add_argument("--epochs", type=int, default=None, help="override CFG.EFF_EPOCHS")
    args = cli.parse_args()

    # runtime config overrides – device & epochs
    if args.device != "auto":
        CFG.DEVICE = args.device  # type: ignore[attr-defined]
    if args.epochs is not None:
        CFG.EFF_EPOCHS = args.epochs  # type: ignore[attr-defined]

    seed_everything(CFG.SEED)
    log = _logger()
    log.info("Device: %s", CFG.DEVICE)

    df = _load_meta()
    if df.empty:
        log.error("No metadata found – run process.py first."); return

    # Determine full class list from metadata (robust to pruning)
    cls_set = {k for js in df["label_json"] for k in json.loads(js)}
    classes = sorted(cls_set)
    s2i = {s: i for i, s in enumerate(classes)}
    log.info("Classes: %d", len(classes))

    # DataLoader – one random 10‑s chunk per source file each epoch
    dataset = MelDataset(df, s2i, augment=True)
    df["_src"] = df["mel_path"].str.extract(r"([^/]+)_\d+s.npy$", expand=False)
    loader = DataLoader(
        dataset,
        batch_size=CFG.EFF_BATCH_SIZE,
        sampler=FileWiseSampler(df, "_src"),
        num_workers=CFG.EFF_NUM_WORKERS,
        pin_memory=CFG.use_cuda(),
        drop_last=True,
    )

    # Ensemble training loop -------------------------------------------------
    for run in range(1, CFG.EFF_NUM_MODELS + 1):
        seed_everything(CFG.SEED + run)  # new seed each run
        trainer = _Trainer(run, classes, log)
        trainer.fit(loader)
        trainer.save(CFG.EFF_MODEL_DIR)
        del trainer; torch.cuda.empty_cache()

    log.info("🏁 Finished – checkpoints in %s", CFG.EFF_MODEL_DIR)


if __name__ == "__main__":
    main()


#### pretrained_models/diffwave-ljspeech/hyperparams.yaml
# ################################################
# Basic parameters for a diffwave vocoder
#
# Author:
#  * Yingzhi Wang 2022
# ################################################

train_timesteps: 50
beta_start: 0.0001
beta_end: 0.05

residual_layers: 30
residual_channels: 64
dilation_cycle_length: 10

unconditional: False

spec_n_mels: 80
spec_hop_length: 256

diffwave: !new:speechbrain.lobes.models.DiffWave.DiffWave
    input_channels: !ref <spec_n_mels>
    residual_layers: !ref <residual_layers>
    residual_channels: !ref <residual_channels>
    dilation_cycle_length: !ref <dilation_cycle_length>
    total_steps: !ref <train_timesteps>
    unconditional: !ref <unconditional>

noise: !new:speechbrain.nnet.diffusion.GaussianNoise

diffusion: !new:speechbrain.lobes.models.DiffWave.DiffWaveDiffusion
    model: !ref <diffwave>
    beta_start: !ref <beta_start>
    beta_end: !ref <beta_end>
    timesteps: !ref <train_timesteps>
    noise: !ref <noise>

modules:
    diffwave: !ref <diffwave>
    diffusion: !ref <diffusion>

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    loadables:
        diffwave: !ref <diffwave>


#### process.py
#!/usr/bin/env python
"""
process.py – BirdCLEF 2025 preprocessing pipeline (updated)
============================================================
Cleans raw *train_audio* / *train_soundscape* audio, applies Voice‑Activity
Detection (VAD), deduplicates, builds **10‑second mel‑spectrogram chunks** with
*soft labels* and sample weighting, then saves:

* `mels/<split>/.../*.npy` – normalized mel spectrogram arrays
* `labels/<split>/.../*.label.npy` – soft‑label vectors
* `<split>_metadata.csv` – one row per chunk

Idempotent: safe to rerun after adding/updating raw data or CFG.
Includes file logging to `process.log` in the processed directory.
"""
from __future__ import annotations

import argparse
import hashlib
import json
import logging
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Sequence

import numpy as np
import pandas as pd
import torch
import timm

from configure import CFG
from data_utils import (
    compute_mel,
    compute_noise_metric,
    load_audio,
    load_vad,
    remove_speech,
    seed_everything,
    segment_audio,
    trim_silence,
)


#──────────────────────────────────────────────────────────────────────────────
# Global constants
#──────────────────────────────────────────────────────────────────────────────
LABEL_W_PRIMARY      = getattr(CFG, "LABEL_WEIGHT_PRIMARY", 0.7)
LABEL_W_SECONDARY      = getattr(CFG, "LABEL_WEIGHT_SECONDARY", 0.2)
LABEL_W_BENCH        = getattr(CFG, "LABEL_WEIGHT_BENCH",    0.1)
RARE_COUNT_THRESHOLD = getattr(CFG, "RARE_COUNT_THRESHOLD", 20)
PSEUDO_WEIGHT        = getattr(CFG, "PSEUDO_WEIGHT",        0.5)


#──────────────────────────────────────────────────────────────────────────────
# Class discovery & benchmark loader (fix state extraction)
#──────────────────────────────────────────────────────────────────────────────

def _discover_classes() -> List[str]:
    if CFG.CLASSES:
        return list(CFG.CLASSES)
    if CFG.TAXONOMY_CSV.exists():
        df = pd.read_csv(CFG.TAXONOMY_CSV)
        if "primary_label" in df:
            return sorted(df["primary_label"].unique())
    if CFG.TRAIN_CSV.exists():
        df = pd.read_csv(CFG.TRAIN_CSV)
        if "primary_label" in df:
            return sorted(df["primary_label"].unique())
    raise RuntimeError("Cannot infer species list; set CFG.CLASSES explicitly")


ALL_CLASSES = _discover_classes()
CLASS2IDX   = {s: i for i, s in enumerate(ALL_CLASSES)}


class BenchmarkModel(torch.nn.Module):
    """Load single trained model for soft-labeling.
    Use the model from the benchmark directory
    directory is: /data/birdclef/models/benchmark/model_fold0.pth

    I used the following code to train the model. Write down the code for loading the model.
    #!/usr/bin/env python
BirdCLEF 2025 Training Pipeline

This script implements the training pipeline for BirdCLEF 2025.
It includes data cleaning (with voice removal), data augmentation,
mel spectrogram extraction, and training of a CNN model (an EfficientNet-B0 sample).

Usage:
  python train_birdclef2025.py train   # to train the model

IMPORTANT:
To ensure training continues after disconnecting from the server, run this script 
within a persistent session such as tmux, screen, or using nohup. For example:

    tmux new -s birdclef_training
    python train_birdclef2025.py train

or

    nohup python train_birdclef2025.py train &

import os
import sys
import glob
import random
import time
import numpy as np
import pandas as pd
import librosa
import cv2
import torch
import torch.nn as nn
import timm
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import logging
import warnings
warnings.filterwarnings("ignore")

# Set up logging to both console and a log file.
logging.basicConfig(level=logging.INFO,
    format='%(asctime)s %(levelname)s:%(message)s',
    handlers=[logging.StreamHandler(),
              logging.FileHandler("training_log.txt", mode='a')])
logger = logging.getLogger()

# -------------------------------
# CONFIGURATION
# -------------------------------
class CFG:
    DATA_DIR = '/data/birdclef'
    TRAIN_AUDIO_DIR = os.path.join(DATA_DIR, 'train_audio')
    TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')
    TAXONOMY_CSV = os.path.join(DATA_DIR, 'taxonomy.csv')
    # Although inference paths exist in the full pipeline, they are not used here.

    SAMPLE_RATE = 32000
    WINDOW_SIZE_SEC = 5           # For inference windows; training uses 10 sec.
    TRAIN_WINDOW_SEC = 10         # Use 10-second chunks for training.
    N_FFT = 1024
    HOP_LENGTH = 512
    N_MELS = 128
    FMIN = 50
    FMAX = 14000
    TARGET_IMG_SIZE = (256, 256)
    
    BATCH_SIZE = 32
    NUM_EPOCHS = 12
    LR = 1e-3
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    NUM_CLASSES = 206  # This will be updated based on the taxonomy.
    
    SEED = 42
    DEBUG = False
    
    # Inference parameters are not used in training.
    use_tta = False  
    tta_count = 3
    threshold = 0.7
    
    use_latent = True  # Latent predictions are typically used only during inference.

CFG = CFG()

def seed_everything(seed=CFG.SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
seed_everything()

# -------------------------------
# DATA LOADING & PREPROCESSING FUNCTIONS
# -------------------------------
def load_audio(path, sr=CFG.SAMPLE_RATE):
    audio, _ = librosa.load(path, sr=sr)
    return audio

def compute_mel_spectrogram(audio, sr=CFG.SAMPLE_RATE):
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sr,
        n_fft=CFG.N_FFT,
        hop_length=CFG.HOP_LENGTH,
        n_mels=CFG.N_MELS,
        fmin=CFG.FMIN,
        fmax=CFG.FMAX,
        power=2.0)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)
    return mel_norm

def resize_spectrogram(melspec, target_size=CFG.TARGET_IMG_SIZE):
    melspec_img = cv2.resize(melspec, target_size, interpolation=cv2.INTER_LINEAR)
    return melspec_img.astype(np.float32)

# -------------------------------
# Voice Activity Detection (VAD)
# -------------------------------
def apply_vad(audio, sr=CFG.SAMPLE_RATE):
    try:
        vad_model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad', trust_repo=True)
        get_speech_timestamps = utils[0]
    except Exception as e:
        logger.error(f"Error loading Silero VAD: {e}")
        return audio
    audio_tensor = torch.tensor(audio, dtype=torch.float32)
    # Pass vad_model as required by the API
    speech_ts = get_speech_timestamps(audio_tensor, vad_model, sampling_rate=sr)
    mask = np.ones(len(audio))
    for seg in speech_ts:
        mask[seg['start']:seg['end']] = 0
    cleaned = audio * mask
    return cleaned

# -------------------------------
# Augmentation Functions
# -------------------------------
def augment_noise_injection(audio, sr=CFG.SAMPLE_RATE, noise_level=(0, 0.5)):
    noise = np.random.randn(len(audio))
    level = np.random.uniform(*noise_level)
    return audio + level * noise

def augment_time_shift(audio, sr=CFG.SAMPLE_RATE, shift_max_sec=2):
    shift = np.random.randint(sr * shift_max_sec)
    direction = np.random.choice([1, -1])
    shifted = np.roll(audio, direction * shift)
    if direction == 1:
        shifted[:shift] = 0
    else:
        shifted[-shift:] = 0
    return shifted

def augment_pitch_shift(audio, sr=CFG.SAMPLE_RATE, n_steps_range=(-2, 2)):
    n_steps = np.random.uniform(*n_steps_range)
    return librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=n_steps)

def augment_time_stretch(audio, sr=CFG.SAMPLE_RATE, rate_range=(0.8, 1.2)):
    rate = np.random.uniform(*rate_range)
    return librosa.effects.time_stretch(y=audio, rate=rate)

def random_augmentation(audio, sr=CFG.SAMPLE_RATE):
    funcs = [augment_noise_injection, augment_time_shift, augment_pitch_shift, augment_time_stretch]
    func = random.choice(funcs)
    return func(audio, sr)

# -------------------------------
# Dataset Class
# -------------------------------
class BirdClefDataset(Dataset):
    def __init__(self, df, audio_dir, species2idx, mode='train', augment=False, use_vad=True, segment_sec=CFG.TRAIN_WINDOW_SEC):
        self.df = df.copy().reset_index(drop=True)
        self.audio_dir = audio_dir
        self.species2idx = species2idx  # mapping from species id (string) to index
        self.mode = mode
        self.augment = augment
        self.use_vad = use_vad
        self.segment_sec = segment_sec
        self.sr = CFG.SAMPLE_RATE
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        file_path = os.path.join(self.audio_dir, row['filename'])
        try:
            audio = load_audio(file_path, sr=self.sr)
        except Exception as e:
            logger.error(f"Error loading {file_path}: {e}")
            audio = np.zeros(int(self.sr * self.segment_sec))
        if self.use_vad and 'Fabio A. Sarria-S' in str(row.get('author', '')):
            audio = apply_vad(audio, sr=self.sr)
        if self.mode == 'train' and self.augment:
            audio = random_augmentation(audio, sr=self.sr)
        required_length = int(self.sr * self.segment_sec)
        if len(audio) < required_length:
            audio = np.pad(audio, (0, required_length - len(audio)), mode='wrap')
        else:
            audio = audio[:required_length]
        melspec = compute_mel_spectrogram(audio, sr=self.sr)
        melspec = resize_spectrogram(melspec, target_size=CFG.TARGET_IMG_SIZE)
        melspec = np.expand_dims(melspec, axis=0)
        
        label = np.zeros(CFG.NUM_CLASSES, dtype=np.float32)
        if 'label' in row and pd.notnull(row['label']):
            sp = str(row['label']).strip()
            if sp in self.species2idx:
                label[self.species2idx[sp]] = 1.0
            else:
                logger.error(f"Label {sp} not found in mapping!")
        elif 'secondary_labels' in row and pd.notnull(row['secondary_labels']):
            parts = str(row['secondary_labels']).strip("[]").replace("'", "").split(',')
            for part in parts:
                sp = part.strip()
                if not sp:
                    continue
                if sp in self.species2idx:
                    label[self.species2idx[sp]] = 1.0
                else:
                    logger.error(f"Secondary label {sp} not found in mapping!")
        return torch.tensor(melspec, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)

# -------------------------------
# Model Definition
# -------------------------------
class BirdCLEFModel(nn.Module):
    def __init__(self, cfg, num_classes=CFG.NUM_CLASSES, backbone_name='efficientnet_b0', in_channels=1):
        super(BirdCLEFModel, self).__init__()
        self.cfg = cfg
        self.backbone = timm.create_model(backbone_name, pretrained=True, in_chans=in_channels,
                                          drop_rate=0.0, drop_path_rate=0.0)
        if hasattr(self.backbone, 'classifier'):
            backbone_out = self.backbone.classifier.in_features
            self.backbone.classifier = nn.Identity()
        elif hasattr(self.backbone, 'fc'):
            backbone_out = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        else:
            backbone_out = self.backbone.get_classifier().in_features
            self.backbone.reset_classifier(0, '')
        self.pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(backbone_out, num_classes)
        
    def forward(self, x):
        feat = self.backbone(x)
        if len(feat.shape) == 4:
            feat = self.pooling(feat)
            feat = feat.view(feat.size(0), -1)
        logits = self.classifier(feat)
        return logits

# -------------------------------
# Training & Validation Functions
# -------------------------------
def train_one_epoch(model, loader, optimizer, criterion, device=CFG.DEVICE):
    model.train()
    losses = []
    for inputs, targets in tqdm(loader, desc='Training', leave=False):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    return np.mean(losses)

def validate_epoch(model, loader, criterion, device=CFG.DEVICE):
    model.eval()
    losses = []
    with torch.no_grad():
        for inputs, targets in tqdm(loader, desc='Validation', leave=False):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            losses.append(loss.item())
    return np.mean(losses)

# -------------------------------
# MAIN: Training Entry Point
# -------------------------------
def main():
    # We only focus on training in this script.
    logger.info("Loading training CSV...")
    df_train = pd.read_csv(CFG.TRAIN_CSV)
    df_train = df_train.drop_duplicates(subset='filename').reset_index(drop=True)
    
    # Build species mapping using taxonomy CSV (fallback to submission header if taxonomy not available).
    try:
        taxonomy_df = pd.read_csv(CFG.TAXONOMY_CSV)
        species_ids = taxonomy_df['primary_label'].astype(str).tolist()
    except Exception as e:
        logger.error("Error reading taxonomy CSV; falling back to sample submission header.")
        sample_sub = pd.read_csv(CFG.SUBMISSION_CSV)
        species_ids = list(sample_sub.columns)[1:]
    
    species2idx = {sp: idx for idx, sp in enumerate(species_ids)}
    CFG.NUM_CLASSES = len(species_ids)
    logger.info(f"Number of classes (after mapping): {CFG.NUM_CLASSES}")
    
    dataset = BirdClefDataset(df_train, CFG.TRAIN_AUDIO_DIR, species2idx, mode='train', augment=True, use_vad=True)
    loader = DataLoader(dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=4)
    
    device = CFG.DEVICE
    model = BirdCLEFModel(CFG, backbone_name='efficientnet_b0').to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LR)
    criterion = nn.BCEWithLogitsLoss()
    best_loss = np.inf
    
    total_start = time.time()
    for epoch in range(1, CFG.NUM_EPOCHS + 1):
        logger.info(f"\nEpoch {epoch}/{CFG.NUM_EPOCHS}")
        epoch_start = time.time()
        
        train_loss = train_one_epoch(model, loader, optimizer, criterion, device=device)
        logger.info(f"Train Loss: {train_loss:.4f}")
        
        val_loss = validate_epoch(model, loader, criterion, device=device)
        logger.info(f"Validation Loss: {val_loss:.4f}")
        
        epoch_duration = time.time() - epoch_start
        remaining_time = (CFG.NUM_EPOCHS - epoch) * epoch_duration
        logger.info(f"Epoch duration: {epoch_duration:.2f} sec, Estimated remaining time: {remaining_time/60:.2f} min")
        
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save({'model_state_dict': model.state_dict()}, 'best_model.pth')
            logger.info("Saved Best Model.")
            
    total_duration = time.time() - total_start
    logger.info(f"\nTraining completed in {total_duration/60:.2f} minutes.")

if __name__ == '__main__':
    main()

    """

#──────────────────────────────────────────────────────────────────────────────
# Utils: dedupe, hashing, save
#──────────────────────────────────────────────────────────────────────────────

def _md5(fp: Path) -> str:
    h = hashlib.md5()
    with fp.open("rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def _deduplicate(paths: Sequence[Path]) -> List[Path]:
    seen, unique = set(), []
    for p in paths:
        try:
            sig = _md5(p)
        except Exception:
            unique.append(p)
            continue
        if sig not in seen:
            seen.add(sig)
            unique.append(p)
    return unique


def _np_save(fp: Path, arr: np.ndarray) -> None:
    fp.parent.mkdir(parents=True, exist_ok=True)
    np.save(fp, arr.astype(np.float32), allow_pickle=False)


#──────────────────────────────────────────────────────────────────────────────
# Soft-label builder
#──────────────────────────────────────────────────────────────────────────────

def _secondary_list(raw) -> List[str]:
    if isinstance(raw, str) and raw:
        return [s for s in raw.split(";") if s]
    return []


def build_soft_label(
    primary: str,
    secondary: str,
    bench_model: Optional[torch.nn.Module] = None,
    wav: Optional[np.ndarray] = None,
) -> Dict[str, float]:
    label = defaultdict(float)
    rem = 1.0 - LABEL_W_BENCH
    if secondaries:
        share = (rem - LABEL_W_PRIMARY) / len(secondaries)
        for s in secondaries:
            label[s] += share
        label[primary] += LABEL_W_PRIMARY
    else:
        label[primary] += rem

    if bench_model and wav is not None:
        mel = compute_mel(wav)
        x = torch.from_numpy(mel).unsqueeze(0).unsqueeze(0)
        with torch.no_grad():
            logits = bench_model(x)
            probs = torch.sigmoid(logits)[0].cpu().numpy()
        for i, p in enumerate(probs):
            if p > 0:
                label[ALL_CLASSES[i]] += LABEL_W_BENCH * p

    total = sum(label.values()) or 1.0
    return {k: v / total for k, v in label.items()}


#──────────────────────────────────────────────────────────────────────────────
# Train-audio processing
#──────────────────────────────────────────────────────────────────────────────

def _process_recordings() -> None:
    log = logging.getLogger()
    log.info("Processing labeled recordings…")

    df = pd.read_csv(CFG.TRAIN_CSV)
    if CFG.MIN_RATING > 0 and "rating" in df:
        df = df[df["rating"] >= CFG.MIN_RATING]

    recs = []
    for r in df.itertuples(index=False):
        fp = CFG.TRAIN_AUDIO_DIR / r.filename
        if not fp.exists():
            log.warning("Missing %s", fp)
            continue
        rec = r._asdict()
        rec["filepath"] = fp
        recs.append(rec)
    df = pd.DataFrame(recs)
    if df.empty:
        log.warning("No recordings found – aborting train stage")
        return

    df = df[df["filepath"].isin(_deduplicate(df["filepath"].tolist()))].reset_index(drop=True)
    df["noise_score"] = [compute_noise_metric(load_audio(fp)) for fp in df["filepath"]]
    ratio = CFG.FOLD0_RATIO
    if 0 < ratio < 1.0:
        thresh = np.quantile(df["noise_score"], ratio)
        df = df[df["noise_score"] <= thresh].reset_index(drop=True)
        log.info("Selected %d clean recordings (%.0f%%)", len(df), 100 * ratio)

    bench = _load_benchmark()
    if bench:
        log.info("Benchmark model loaded for smoothing and swap logic.")
    vad_model, vad_ts = load_vad()

    mel_root = CFG.PROCESSED_DIR / "mels" / "train"
    lbl_root = CFG.PROCESSED_DIR / "labels" / "train"

    rows = []
    for rec in df.itertuples(index=False):
        y = load_audio(rec.filepath)
        y = trim_silence(y)
        y = remove_speech(y, vad_model, vad_ts)
        if np.sqrt((y**2).mean()) < CFG.RMS_THRESHOLD:
            continue

        segments = list(segment_audio(y))
        secondaries = _secondary_list(getattr(rec, "secondary_labels", ""))
        primary_lbl = rec.primary_label

        if bench:
            mel_full = compute_mel(y)
            x_full = torch.from_numpy(mel_full).unsqueeze(0).unsqueeze(0)
            with torch.no_grad():
                logits_full = bench(x_full)
                probs_full = torch.sigmoid(logits_full)[0].cpu().numpy()
            top_lbl = ALL_CLASSES[int(probs_full.argmax())]
            if top_lbl != primary_lbl:
                if top_lbl in secondaries:
                    log.info("Swapping primary %s -> %s", primary_lbl, top_lbl)
                    primary_lbl = top_lbl
                else:
                    continue

        for start_sec, chunk in segments:
            L = CFG.TRAIN_CHUNK_SEC * CFG.SAMPLE_RATE
            if len(chunk) < L:
                chunk = np.pad(chunk, (0, L - len(chunk)), mode="wrap")
            mel_chunk = compute_mel(chunk)
            soft = build_soft_label(primary_lbl, secondaries, bench, chunk)

            stem = f"{rec.filepath.stem}_{int(start_sec)}s"
            species_str = str(primary_lbl)
            mp = mel_root / species_str / f"{stem}.npy"
            lp = lbl_root / species_str / f"{stern}.label.npy"
            _np_save(mp, mel_chunk)
            vec = np.array([soft.get(c, 0.0) for c in ALL_CLASSES], dtype=np.float32)
            _np_save(lp, vec)

            rows.append({
                "mel_path": str(mp.relative_to(CFG.PROCESSED_DIR)),
                "label_path": str(lp.relative_to(CFG.PROCESSED_DIR)),
                "label_json": json.dumps(soft, separators=(",",":")),
                "duration": CFG.TRAIN_CHUNK_SEC,
                "noise_score": rec.noise_score,
                "weight": 1.0,
            })

    out_csv = CFG.PROCESSED_DIR / "train_metadata.csv"
    pd.DataFrame(rows).to_csv(out_csv, index=False)
    log.info("Saved %d mel chunks -> %s", len(rows), out_csv.name)


#──────────────────────────────────────────────────────────────────────────────
# Soundscape pseudo-labelling
#──────────────────────────────────────────────────────────────────────────────
def _process_soundscapes() -> None:
    log = logging.getLogger()
    if not CFG.TRAIN_SOUNDSCAPE_DIR.exists():
        log.info("No soundscape directory – skipping pseudo-labelling stage")
        return

    bench = _load_benchmark()
    if bench is None:
        log.warning("Benchmark unavailable – using uniform pseudo-labels")

    vad_model, vad_ts = load_vad()
    mel_root = CFG.PROCESSED_DIR / "mels" / "soundscape"
    lbl_root = CFG.PROCESSED_DIR / "labels" / "soundscape"

    rows = []
    for fp in sorted(CFG.TRAIN_SOUNDSCAPE_DIR.glob("*.ogg")):
        y = load_audio(fp)
        y = trim_silence(y)
        y = remove_speech(y, vad_model, vad_ts)

        for start_sec, seg in segment_audio(y):
            L = CFG.TRAIN_CHUNK_SEC * CFG.SAMPLE_RATE
            if len(seg) < L:
                seg = np.pad(seg, (0, L - len(seg)), mode="wrap")
            mel_seg = compute_mel(seg)
            if bench:
                x = torch.from_numpy(mel_seg).unsqueeze(0).unsqueeze(0)
                with torch.no_grad():
                    probs = torch.sigmoid(bench(x))[0].cpu().numpy()
            else:
                probs = np.ones(len(ALL_CLASSES), dtype=np.float32)

            if float(probs.max()) < CFG.PSEUDO_THRESHOLD:
                continue

            stem = f"{fp.stem}_{int(start_sec)}s"
            mp = mel_root / f"{stem}.npy"
            lp = lbl_root / f"{stem}.label.npy"
            _np_save(mp, mel_seg)
            _np_save(lp, probs)

            soft = {ALL_CLASSES[i]: float(v) for i, v in enumerate(probs) if v > 0}
            rows.append({
                "mel_path": str(mp.relative_to(CFG.PROCESSED_DIR)),
                "label_path": str(lp.relative_to(CFG.PROCESSED_DIR)),
                "label_json": json.dumps(soft, separators=(",",":")),
                "duration": CFG.TRAIN_CHUNK_SEC,
                "noise_score": compute_noise_metric(seg),
                "weight": PSEUDO_WEIGHT,
            })

    if rows:
        out_csv = CFG.PROCESSED_DIR / "soundscape_metadata.csv"
        pd.DataFrame(rows).to_csv(out_csv, index=False)
        log.info("Saved %d pseudo-labelled chunks -> %s", len(rows), out_csv.name)


#──────────────────────────────────────────────────────────────────────────────
# Rare-species weighting
#──────────────────────────────────────────────────────────────────────────────
def _apply_rare_weighting() -> None:
    files = [
        CFG.PROCESSED_DIR / "train_metadata.csv",
        CFG.PROCESSED_DIR / "soundscape_metadata.csv",
    ]
    dfs = [pd.read_csv(f) for f in files if f.exists()]
    if not dfs:
        return
    all_df = pd.concat(dfs, ignore_index=True)
    counts = Counter(
        max(json.loads(js), key=json.loads(js).get)
        for js in all_df["label_json"]
    )
    rare_species = {s for s, c in counts.items() if c < RARE_COUNT_THRESHOLD}

    for f in files:
        if not f.exists():
            continue
        df = pd.read_csv(f)
        df["weight"] = [
            CFG.RARE_WEIGHT if max(json.loads(js), key=json.loads(js).get) in rare_species else 1.0
            for js in df["label_json"]
        ]
        df.to_csv(f, index=False)
    logging.getLogger().info(
        "Applied rare-species weighting (x%.1f) to %d species",
        CFG.RARE_WEIGHT,
        len(rare_species),
    )


#──────────────────────────────────────────────────────────────────────────────
# Entrypoint with file logging
#──────────────────────────────────────────────────────────────────────────────
def main() -> None:
    p = argparse.ArgumentParser(description="Preprocess BirdCLEF 2025 audio")
    p.add_argument(
        "--verbose", action="store_true", help="debug logging"
    )
    args = p.parse_args()
    lvl = logging.DEBUG if args.verbose else logging.INFO

    # Ensure processed directory exists (for data and logs)
    CFG.PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

    # Configure logging to console and file
    log_file = CFG.PROCESSED_DIR / "process.log"
    handlers = [logging.StreamHandler(), logging.FileHandler(log_file, mode='a', encoding='utf-8')]
    logging.basicConfig(
        level=lvl,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=handlers,
    )

    seed_everything(CFG.SEED)
    logging.getLogger().info("Starting preprocessing; logs writing to %s", log_file)

    _process_recordings()
    _process_soundscapes()
    _apply_rare_weighting()

    logging.getLogger().info("✅ Preprocessing finished – data saved to %s", CFG.PROCESSED_DIR)

if __name__ == "__main__":
    main()


#### regnety.py
#!/usr/bin/env python
"""
regnety.py – BirdCLEF 2025 trainer (fixed)
========================================
Trains **CFG.REG_NUM_MODELS** class‑conditional RegNetY‑0.8GF models on the
pre‑processed mel‑spectrogram chunks.  This revision fixes the CSV‑loading
bugs flagged in the review:

* **Uses `pd.read_csv` / `pd.concat`** instead of the erroneous
  `Path.read_csv()` call.
* Adds the missing **`import pandas as pd`** statement.
* Keeps all earlier improvements (model‑name guard, CUDA pin‑memory toggle,
  checkpoint arch string, seed offset, CLI alias, etc.).
"""
from __future__ import annotations

import argparse
import json
import logging
from pathlib import Path
from typing import Dict, List

import pandas as pd  # <‑‑‑ FIXED: explicit pandas import
import torch
import torch.nn.functional as F
import timm
from torch import optim
from torch.cuda.amp import GradScaler, autocast
from torch.utils.data import DataLoader

from configure import CFG
from data_utils import FileWiseSampler, MelDataset, seed_everything

# ────────────────────────────────────────────────────────────────────────
# Helpers
# ────────────────────────────────────────────────────────────────────────

def _get_model_name() -> str:
    for name in ("regnety_008", "regnety_008gf"):
        if name in timm.list_models():
            return name
    raise RuntimeError("RegNetY‑0.8GF backbone missing in timm build.")


def _soft_ce(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    if targets.dtype == torch.long:
        return F.cross_entropy(logits, targets, reduction="none")
    return -(targets * torch.log_softmax(logits, dim=1)).sum(1)


def _logger() -> logging.Logger:
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s [%(levelname)s] %(message)s")
    return logging.getLogger("regnety")

# ────────────────────────────────────────────────────────────────────────
# Trainer
# ────────────────────────────────────────────────────────────────────────

class _Trainer:
    def __init__(self, rid: int, n_cls: int, arch: str, log: logging.Logger):
        self.rid, self.log = rid, log
        self.device = torch.device("cuda" if CFG.use_cuda() else "cpu")
        self.model = timm.create_model(arch, pretrained=True, in_chans=1,
                                       num_classes=n_cls).to(self.device)
        self.opt = optim.AdamW(self.model.parameters(), lr=CFG.REG_LR,
                               weight_decay=CFG.REG_WEIGHT_DECAY)
        self.sched = optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=CFG.REG_EPOCHS)
        self.scaler = GradScaler(enabled=CFG.use_cuda())

    def _step(self, batch):
        x, y, w = (t.to(self.device, non_blocking=True) for t in batch)
        self.opt.zero_grad(set_to_none=True)
        with autocast(enabled=CFG.use_cuda()):
            loss = (_soft_ce(self.model(x), y) * w).mean()
        self.scaler.scale(loss).backward(); self.scaler.step(self.opt); self.scaler.update()
        return float(loss.detach())

    def fit(self, loader: DataLoader):
        for ep in range(1, CFG.REG_EPOCHS + 1):
            self.model.train(); run = 0.0
            for batch in loader:
                run += self._step(batch) * batch[0].size(0)
            self.sched.step()
            self.log.info("run=%d ep=%d/%d loss=%.5f lr=%.2e", self.rid, ep,
                          CFG.REG_EPOCHS, run / len(loader.dataset), self.sched.get_last_lr()[0])

    def save(self, out: Path, arch: str, s2i: Dict[str, int]):
        out.mkdir(parents=True, exist_ok=True)
        torch.save({"arch": arch, "model": self.model.state_dict(), "species2idx": s2i},
                   out / f"{arch}_run{self.rid}.pth")
        self.log.info("✔ saved checkpoint run%d", self.rid)

# ────────────────────────────────────────────────────────────────────────
# Data helpers
# ────────────────────────────────────────────────────────────────────────

def _load_meta() -> pd.DataFrame:  # <‑‑‑ FIXED: correct pandas usage
    df = pd.read_csv(CFG.PROCESSED_DIR / "train_metadata.csv")
    sc = CFG.PROCESSED_DIR / "soundscape_metadata.csv"
    if sc.exists():
        df = pd.concat([df, pd.read_csv(sc)], ignore_index=True)
    return df

# ────────────────────────────────────────────────────────────────────────
# Main
# ────────────────────────────────────────────────────────────────────────

def main() -> None:
    cli = argparse.ArgumentParser(description="Train RegNetY‑0.8GF ensemble")
    cli.add_argument("--device", choices=["auto", "cpu", "gpu", "cuda"], default="auto")
    args = cli.parse_args()
    if args.device != "auto":
        CFG.DEVICE = args.device  # type: ignore[attr-defined]

    seed_everything(CFG.SEED)
    log = _logger()
    log.info("Device: %s", CFG.DEVICE)

    df = _load_meta()
    if df.empty:
        log.error("No metadata – run process.py first."); return

    cls_set = {k for js in df["label_json"] for k in json.loads(js)}
    classes = sorted(cls_set); s2i = {s: i for i, s in enumerate(classes)}
    log.info("Classes: %d", len(classes))

    data = MelDataset(df, s2i, augment=True)
    df["_src"] = df["mel_path"].str.extract(r"([^/]+)_\d+s.npy$", expand=False)
    loader = DataLoader(data, batch_size=CFG.REG_BATCH_SIZE,
                        sampler=FileWiseSampler(df, "_src"),
                        num_workers=CFG.REG_NUM_WORKERS,
                        pin_memory=CFG.use_cuda(), drop_last=True)

    arch = _get_model_name()
    for run in range(1, CFG.REG_NUM_MODELS + 1):
        seed_everything(CFG.SEED + 1000 + run)
        t = _Trainer(run, len(classes), arch, log); t.fit(loader); t.save(CFG.REG_MODEL_DIR, arch, s2i)
        del t; torch.cuda.empty_cache()

    log.info("🏁 Finished – checkpoints in %s", CFG.REG_MODEL_DIR)


if __name__ == "__main__":
    main()


#### test/test_datautils.py
import sys
from pathlib import Path
import json
import random
import numpy as np
import torch
import pandas as pd
import pytest

# ensure project root is importable
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
import configure
import data_utils as du

@pytest.fixture(autouse=True)
def deterministic_seed():
    # Ensure reproducible randomness
    du.seed_everything(123)
    return None

def test_seed_everything():
    # Test that random, numpy, and torch seeds are consistent
    du.seed_everything(42)
    a = random.random()
    b = np.random.rand()
    c = torch.rand(1).item()

    du.seed_everything(42)
    assert random.random() == pytest.approx(a)
    assert np.random.rand() == pytest.approx(b)
    assert torch.rand(1).item() == pytest.approx(c)

def test_compute_noise_metric():
    # Create a known array
    y = np.array([1.0, -1.0, 2.0], dtype=np.float32)
    # compute std, var, rms, sum
    expected = float(y.std() + y.var() + np.sqrt((y**2).mean()) + (y**2).sum())
    assert du.compute_noise_metric(y) == pytest.approx(expected)

def test_spec_augment_and_cutmix():
    # Create dummy mel array
    mel = np.ones((10, 20), dtype=np.float32)
    # Spec augment should maintain shape
    mel_aug = du.spec_augment(mel, freq_mask_param=2, time_mask_param=3, num_masks=1)
    assert mel_aug.shape == mel.shape
    # At least some zeros appear in augmented mel
    assert np.any(mel_aug == 0.0)

    # Test cutmix merges two arrays
    m1 = np.ones((5, 10), dtype=np.float32)
    l1 = torch.tensor([1.0, 0.0])
    m2 = np.zeros((5, 10), dtype=np.float32)
    l2 = torch.tensor([0.5, 0.5])
    mixed, label = du.cutmix(m1, l1, m2, l2)
    assert mixed.shape == m1.shape
    assert isinstance(label, torch.Tensor)

def test_segment_audio():
    # Create an array of length 2 seconds at 10 Hz sample
    sr = 10
    y = np.arange(sr * 2).astype(np.float32)
    # override CFG for chunk/hop secs
    configure.CFG.TRAIN_CHUNK_SEC = 1
    configure.CFG.TRAIN_CHUNK_HOP_SEC = 1
    configure.CFG.SAMPLE_RATE = sr
    segments = list(du.segment_audio(y, sr=sr))
    assert len(segments) == 2
    for start_sec, chunk in segments:
        assert len(chunk) == sr * 1

def test_load_and_trim_audio(monkeypatch):
    # Stub librosa.load and trim functions
    called = {}

    def fake_load(fp, sr, mono=True):
        called['load'] = (fp, sr, mono)
        return np.array([0.1, -0.1], dtype=np.float32), sr
    monkeypatch.setattr(du.librosa, 'load', fake_load)

    def fake_trim(y, top_db):
        called['trim'] = (y.tolist(), top_db)
        return y[:1], (0,)
    monkeypatch.setattr(du.librosa.effects, 'trim', fake_trim)

    y = du.load_audio('dummy.wav', sample_rate=16000)
    assert isinstance(y, np.ndarray)
    y_trim = du.trim_silence(y)
    assert isinstance(y_trim, np.ndarray)
    assert 'load' in called and 'trim' in called

def test_compute_mel_cpu(monkeypatch):
    # Stub librosa.feature and power_to_db
    mel_out = np.ones((4, 5), dtype=np.float32)
    monkeypatch.setattr(du.librosa.feature, 'melspectrogram', lambda **kw: mel_out)
    monkeypatch.setattr(du.librosa, 'power_to_db', lambda m, ref: m)
    # Override CFG params
    configure.CFG.SAMPLE_RATE = 16000
    configure.CFG.HOP_LENGTH = 128
    configure.CFG.N_FFT = 512
    configure.CFG.N_MELS = 4
    configure.CFG.FMIN = 0
    configure.CFG.FMAX = 8000
    configure.CFG.POWER = 2.0
    y = np.random.randn(100).astype(np.float32)
    mel = du.compute_mel(y, to_db=True)
    assert np.allclose(mel, mel_out)

def test_FileWiseSampler():
    df = pd.DataFrame({'filepath': ['a.wav', 'a.wav', 'b.wav']})
    sampler = du.FileWiseSampler(df, 'filepath')
    indices = list(iter(sampler))
    assert len(indices) == 2
    assert set(indices).issubset({0, 1, 2})

def test_MelDataset(monkeypatch, tmp_path):
    # Prepare fake processed directory
    processed = tmp_path / 'processed'
    processed.mkdir()
    # Create dummy mel and label files
    mel_dir = processed / 'mels' / 'train' / 'sp'
    label_dir = processed / 'labels' / 'train' / 'sp'
    mel_dir.mkdir(parents=True)
    label_dir.mkdir(parents=True)
    mel_arr = np.ones((2, 2), dtype=np.float32)
    np.save(mel_dir / 'p1.npy', mel_arr)
    label_vec = np.array([1.0], dtype=np.float32)
    np.save(label_dir / 'p1.label.npy', label_vec)

    monkeypatch.setattr(configure.CFG, 'PROCESSED_DIR', processed)
    monkeypatch.setattr(configure.CFG, 'TARGET_SHAPE', (2, 2))
    configure.CFG.USE_SOFT_LABELS = True

    # cv2.resize identity stub
    monkeypatch.setattr(du.cv2, 'resize', lambda img, shape, interpolation: img)

    df = pd.DataFrame({
        'mel_path': ['mels/train/sp/p1.npy'],
        'label_path': ['labels/train/sp/p1.label.npy'],
    })
    ds = du.MelDataset(df, {'sp': 0}, augment=False)
    mel_tensor, label, weight = ds[0]
    assert isinstance(mel_tensor, torch.Tensor)
    assert mel_tensor.shape[1:] == mel_arr.shape
    assert isinstance(label, torch.Tensor)
    assert weight == 1.0


#### test/test_diffwave.py
import sys
from pathlib import Path
import pytest
import numpy as np
import torch
import pandas as pd

# Ensure project root is on path
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
import configure
import diffwave


@pytest.fixture(autouse=True)
def setup_diffwave_env(tmp_path, monkeypatch):
    # Create test root and data directories
    root = tmp_path / 'test'
    root.mkdir()

    # Raw audio structure
    data_dir = root / 'data'
    audio_dir = data_dir / 'train_audio' / '41970'
    audio_dir.mkdir(parents=True)
    # Create placeholder audio file
    (audio_dir / 'iNat327629.ogg').write_bytes(b'')

    # Create train.csv for plan
    train_csv = data_dir / 'train.csv'
    train_csv.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame([
        {'primary_label': '41970', 'filename': 'iNat327629.ogg'}
    ]).to_csv(train_csv, index=False)

    # Monkey-patch CFG to point at our dirs
    monkeypatch.setattr(configure.CFG, 'TRAIN_AUDIO_DIR', data_dir / 'train_audio')
    monkeypatch.setattr(configure.CFG, 'TRAIN_CSV', train_csv)
    processed_dir = root / 'processed'
    monkeypatch.setattr(configure.CFG, 'PROCESSED_DIR', processed_dir)
    processed_dir.mkdir()

    # Create dummy mel-spectrograms under processed_dir/mels/train/41970/*
    mel_dir = processed_dir / 'mels' / 'train' / '41970'
    mel_dir.mkdir(parents=True, exist_ok=True)
    dummy_mel = np.random.rand(configure.CFG.N_MELS, 10).astype(np.float32)
    np.save(mel_dir / 'chunk0.npy', dummy_mel)

    # Stub out the actual DiffWaveVocoder so we don't need real model
    class DummyVocoder:
        @classmethod
        def from_hparams(cls, *args, **kwargs):
            return cls()
        def decode_spectrogram(self, mel_tensor, hop_length, fast_sampling, fast_sampling_noise_schedule):
            # Return a silent waveform whose length matches frames*hop_length
            frames = mel_tensor.shape[-1]
            length = frames * hop_length
            return torch.zeros((1, length), dtype=torch.float32)

    monkeypatch.setattr(diffwave, 'DiffWaveVocoder', DummyVocoder)

    return {
        'audio_dir': audio_dir,
        'train_csv': train_csv
    }


def test_diffwave_generate_and_patch(setup_diffwave_env, monkeypatch):
    ctx = setup_diffwave_env

    # Simulate CLI invocation
    monkeypatch.setattr(sys, 'argv', ['diffwave.py', 'generate'])
    diffwave.main()

    # Expect exactly one synthetic file in the species directory
    out_file = ctx['audio_dir'] / 'synthetic_000.ogg'
    assert out_file.exists(), f"Expected synthetic file at {out_file}"

    # Check that train.csv has been patched with the new entry
    df = pd.read_csv(ctx['train_csv'])
    entries = set(zip(df['primary_label'], df['filename']))
    assert ('41970', 'synthetic_000.ogg') in entries, \
        "New synthetic entry not found in train.csv"


#### test/test_process.py
import sys
import logging
import shutil
from pathlib import Path

import pytest
import pandas as pd

import configure
import process

# Ensure project root is on sys.path
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

@pytest.fixture(autouse=True)
def setup_test_env(tmp_path, monkeypatch):
    # Setup temporary data roots
    root = tmp_path / "test"
    data_dir = root / "data"
    audio_dir = data_dir / "train_audio" / "41970"
    soundscape_dir = data_dir / "train_soundscapes"
    audio_dir.mkdir(parents=True)
    soundscape_dir.mkdir(parents=True)

    # Copy sample .ogg files from dataset
    src_audio = Path("/data/birdclef/train_audio/41970/iNat327629.ogg")
    src_sc = Path("/data/birdclef/train_soundscapes/H02_20230420_074000.ogg")
    shutil.copy(src_audio, audio_dir / "iNat327629.ogg")
    shutil.copy(src_sc, soundscape_dir / "H02_20230420_074000.ogg")

    # Create train.csv and taxonomy.csv
    train_csv = data_dir / "train.csv"
    train_csv.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame([
        {"primary_label": "41970", "filename": "41970/iNat327629.ogg"}
    ]).to_csv(train_csv, index=False)
    tax_csv = data_dir / "taxonomy.csv"
    pd.DataFrame([{"primary_label": "41970"}]).to_csv(tax_csv, index=False)

    # Monkeypatch CFG paths
    monkeypatch.setattr(configure.CFG, 'TRAIN_AUDIO_DIR', data_dir / 'train_audio')
    monkeypatch.setattr(configure.CFG, 'TRAIN_SOUNDSCAPE_DIR', soundscape_dir)
    monkeypatch.setattr(configure.CFG, 'TRAIN_CSV', train_csv)
    monkeypatch.setattr(configure.CFG, 'TAXONOMY_CSV', tax_csv)
    processed_dir = root / 'processed'
    monkeypatch.setattr(configure.CFG, 'PROCESSED_DIR', processed_dir)
    # No benchmark model
    monkeypatch.setattr(configure.CFG, 'BENCHMARK_MODEL', None)

    return root


def test_process_pipeline(setup_test_env, caplog):
    caplog.set_level(logging.INFO)
    # Run preprocessing
    process.main()

    proc = configure.CFG.PROCESSED_DIR

    # Train recordings outputs
    mel_train = proc / 'mels' / 'train' / '41970'
    lbl_train = proc / 'labels' / 'train' / '41970'
    assert mel_train.exists() and any(mel_train.iterdir()), f"No mel files in {mel_train}"
    assert lbl_train.exists() and any(lbl_train.iterdir()), f"No label files in {lbl_train}"

    # Soundscape outputs
    mel_sc = proc / 'mels' / 'soundscape'
    lbl_sc = proc / 'labels' / 'soundscape'
    assert mel_sc.exists() and any(mel_sc.iterdir()), f"No soundscape mels in {mel_sc}"
    assert lbl_sc.exists() and any(lbl_sc.iterdir()), f"No soundscape labels in {lbl_sc}"

    # Metadata files
    meta_train = proc / 'train_metadata.csv'
    meta_sc = proc / 'soundscape_metadata.csv'
    assert meta_train.exists(), "train_metadata.csv missing"
    assert meta_sc.exists(), "soundscape_metadata.csv missing"

    # Log check
    assert "Processing labelled recordings" in caplog.text


